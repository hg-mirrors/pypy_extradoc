\section{Benchmarks}

\anto{We should say somewhere that flexswitches are slow but benchmarks are so
  good because they are not involved in the inner loops}

In section \ref{sec:tlc-properties}, we saw that TLC provides most of the
features that usaully make dynamically typed language so slow, such as
\emph{stack-based interpreter}, \emph{boxed arithmetic} and \emph{dynamic lookup} of
methods and attributes.

In the following sections, we will show some benchmarks that show how our
generated JIT can handle all these features very well.

To measure the speedup we get with the JIT, we run each program three times:

\begin{enumerate}
\item By plain interpretation, without any jitting.
\item With the JIT enabled: this run includes the time spent by doing the
  compilation itself, plus the time spent by running the produced code.
\item Again with the JIT enabled, but this time the compilation has already
  been done, so we are actually measuring how good is the code we produced.
\end{enumerate}

Moreover, for each benchmark we also show the time taken by running the
equivalent program written in C\#.  By comparing the results against C\#, we
can see how far we are from the supposed optimal performances.  \anto{I
  don't really like the last sentence, but right now I can't think of another
  way to phrase it.  Rewording welcome :-)}

The benchmarks have been run on machine with Intel Pentium 4 CPU running at
3.20 GHz and 2 GB of RAM, running Microsoft Windows XP and Microsoft .NET
Framework 2.0.

\subsection{Arithmetic operations}

To benchmark arithmetic operations between integers, we wrote a simple program
that computes the factorial of a given number.  The algorithm is
straightforward, and the loop contains only three operations: one
multiplication, one subtraction, and one comparison to check if we have
finished the job:

\begin{lstlisting}
def main(n):
    result = 1
    while n > 1:
        result = result * n
        n = n - 1
    return n
\end{lstlisting}

When doing plain interpretation, we need to create and destroy three temporary
objects at each iterations.  By contrast, the code generated by the JIT does
much better.  At the first iteration, the classes of the two operands of the
multiplication are promoted; then, the JIT compiler knows that both are
integers, so it can inline the code to compute the result.  Moreover, it can
\emph{virtualize} (see section \ref{sec:virtuals} all the temporary objects, because they never escape from
the inner loop.  The same remarks apply to the other two operations inside
the loop.

As a result, the code executed after the first iteration is close to optimal:
the intermediate values are stored as \lstinline{int} local variables, and the
multiplication, subtraction and \emph{less-than} comparison are mapped to a
single CLI opcode (\lstinline{mul}, \lstinline{sub} and \lstinline{clt},
respectively).

Similarly, we wrote a program to calculate the $n_{th}$ Fibonacci number, for
which we can do the same reasoning as above:

\begin{lstlisting}
def main(n):
    a = 0
    b = 1
    while n > 1:
        sum = a + b
        a = b
        b = sum
        n = n - 1
    return b
\end{lstlisting}

\anto{these tables are ugly}

\begin{table}[ht]
  \begin{tabular}{|l|r|r|r|r||r|r|}
    \hline
    \textbf{n} & 
    \textbf{Interp} &
    \textbf{JIT} &
    \textbf{JIT 2} &
    \textbf{C\#} &
    \textbf{Interp/JIT 2} &
    \textbf{JIT 2/C\#} \\
    \hline

    $10$    &   0.031  &  0.422  &  0.000  &  0.000  &      N/A  &    N/A \\
    $10^7$  &  30.984  &  0.453  &  0.047  &  0.031  &  661.000  &  1.500 \\
    $10^8$  &     N/A  &  0.859  &  0.453  &  0.359  &      N/A  &  1.261 \\
    $10^9$  &     N/A  &  4.844  &  4.641  &  3.438  &      N/A  &  1.350 \\

    \hline

  \end{tabular}
  \caption{Factorial benchmark}
  \label{tab:factorial}
\end{table}


\begin{table}[ht]
  \begin{tabular}{|l|r|r|r|r||r|r|}
    \hline
    \textbf{n} & 
    \textbf{Interp} &
    \textbf{JIT} &
    \textbf{JIT 2} &
    \textbf{C\#} &
    \textbf{Interp/JIT 2} &
    \textbf{JIT 2/C\#} \\
    \hline

    $10$    &   0.031  &  0.453  &  0.000  &  0.000  &       N/A  &  N/A   \\
    $10^7$  &  29.359  &  0.469  &  0.016  &  0.016  &  1879.962  &  0.999 \\
    $10^8$  &     N/A  &  0.688  &  0.250  &  0.234  &       N/A  &  1.067 \\
    $10^9$  &     N/A  &  2.953  &  2.500  &  2.453  &       N/A  &  1.019 \\

    \hline

  \end{tabular}
  \caption{Fibonacci benchmark}
  \label{tab:fibo}
\end{table}


Tables \ref{tab:factorial} and \ref{tab:fibo} show the time spent to calculate
the factorial and Fibonacci for various $n$.  As we can see, for small values
of $n$ the time spent running the JIT compiler is much higher than the time
spent to simply interpret the program.  This is an expected result, as till
now we only focused on optimizing the compiled code, not the compilation
process itself.

On the other hand, to get meaningful timings we had to use very high values of
$n$.  This means that the results are incorrect due to overflow, but since all
the runnings overflow in the very same way, the timings are still
comparable. \anto{I think we should rephrase this sentence}.  For $n$ greater
than $10^7$, we did not run the interpreted program as it would have took too
much time, without adding anything to the discussion.

As we can see, the code generated by the JIT can be up to ~1800 times faster
than the non-jitted case.  Moreover, it often runs at the same speed as the
equivalent program written in C\#, being only 1.5 slower in the worst case.

The difference in speed it is probably due to both the fact that the current
CLI backend emits slightly non-optimal code and that the underyling .NET JIT
compiler is highly optimized to handle bytecode generated by C\# compilers.

\subsection{Object-oriented features}

To measure how the JIT handles object-oriented features, we wrote a very
simple benchmark that involves attribute lookups and polymorphic method calls:

\begin{lstlisting}
def main(n):
    if n < 0:
        n = -n
        obj = new(value, accumulate=count)
    else:
        obj = new(value, accumulate=add)
    obj.value = 0
    while n > 0:
        n = n - 1
        obj.accumulate(n)
    return obj.value

def count(x):
    this.value = this.value + 1

def add(x):
    this.value = this.value + x
\end{lstlisting}

The two \lstinline{new} operations create an object with exactly one field
\lstinline{value} and one method \lstinline{accumulate}, whose implementation
is found in the functions \lstinline{count} and \lstinline{add}, respectively.
When calling a method, the receiver is implicity passed and can be accessed
through the special name \lstinline{this}.

The computation \emph{per se} is trivial, as it calculates either $-n$ or
$1+2...+n-1$, depending on the sign of $n$. The interesting part is the
polymorphic call to \lstinline{accumulate} inside the loop, because the interpreter has
no way to know in advance which method to call (unless it does flow analysis,
which could be feasible in this case but not in general).  The equivalent C\#
code we wrote uses two classes and a \lstinline{virtual} method call to
implement this behaviour.

However, our generated JIT does not compile the whole function at
once. Instead, it compiles and executes code chunk by chunk, waiting until it
knows enough informations to generate highly efficient code.  In particualr,
at the time it emits the code for the inner loop it exactly knows the
type of \lstinline{obj}, thus it can remove the overhead of dynamic dispatch
and inline the method call.  Moreover, since \lstinline{obj} never escapes the
function, it is \emph{virtualized} and its field \lstinline{value} is stored
as a local variable.  As a result, the generated code results in a simple loop
doing additions in-place.

\begin{table}[ht]
  \begin{tabular}{|l|r|r|r|r||r|r|}
    \hline
    \textbf{n} & 
    \textbf{Interp} &
    \textbf{JIT} &
    \textbf{JIT 2} &
    \textbf{C\#} &
    \textbf{Interp/JIT 2} &
    \textbf{JIT 2/C\#} \\
    \hline

    $10$    &   0.031  &  0.453  &  0.000  &  0.000  &      N/A  &  N/A   \\
    $10^7$  &  43.063  &  0.516  &  0.047  &  0.063  &  918.765  &  0.750 \\
    $10^8$  &     N/A  &  0.875  &  0.453  &  0.563  &      N/A  &  0.806 \\
    $10^9$  &     N/A  &  4.188  &  3.672  &  5.953  &      N/A  &  0.617 \\

    \hline

  \end{tabular}
  \caption{Accumulator benchmark}
  \label{tab:accumulator}
\end{table}


Table \ref{tab:accumulator} show the results for the benchmark.  Again, we can
see that the speedup of the JIT over the interpreter is comparable to the
other two benchmarks.  However, the really interesting part is the comparison
with the equivalent C\# code, as the code generated by the JIT is
\textbf{faster}.

Probably, the C\# code is slower because:

\begin{itemize}
\item The object is still allocated on the heap, and thus there is an extra
  level of indirection to access the \lstinline{value} field.
\item The method call is optimized through a \emph{polymorphic inline cache}
  \cite{hoelzle_optimizing_1991}, that requires a guard check at each iteration.
\end{itemize}

\anto{maybe we should move the following paragraph to
  abstract/introduction/conclusion?}

Despite being only a microbenchmark, this result is very important as it proves
that our strategy of intermixing compile time and runtime can yield to better
performances than current techniques.  The result is even more impressive if
we consider dynamically typed languages as TLC are usually considered much
slower than the statically typed ones.
