\section{Benchmarks}

In section \ref{sec:tlc-features}, we saw that TLC provides most of the
features that usaully make dynamically typed language so slow, such as
\emph{stack-based VM}, \emph{boxed arithmetic} and \emph{dynamic lookup} of
methods and attributes.

In the following sections, we will show some benchmarks that show how our
generated JIT can handle all these features very well.

To measure the speedup we get with the JIT, we run each program three times:

\begin{enumerate}
\item By plain interpretation, without any jitting.
\item With the jit enabled: this run includes the time spent by doing the
  compilation itself, plus the time spent by running the produced code.
\item Again with the jit enabled, but this time the compilation has already
  been done, so we are actually measuring how good is the code we produced.
\end{enumerate}

Moreover, for each benchmark we also show the time taken by running the
equivalent program written in C\#.  By comparing the results against C\#, we
can see how far we are from the supposed optimal performances.  \anto{I
  don't really like the last sentence, but right now I can't think of another
  way to phrase it.  Rewording welcome :-)}

The benchmarks have been run on machine XXX with hardware YYY etc. etc.

\subsection{Arithmetic operations}

To benchmark arithmetic operations between integers, we wrote a simple program
that computes the factorial of a given number.  The algorithm is
straightforward, and the loop contains only three operations: one
multiplication, one subtraction, and one comparison to check if we have
finished the job.

When doing plain interpretation, we need to create and destroy three temporary
objects at each iterations.  By contrast, the code generated by the JIT does
much better.  At the first iteration, the classes of the two operands of the
multiplication are promoted; then, the JIT compiler knows that both are
integers, so it can inline the code to compute the result.  Moreover, it can
\emph{virtualize} all the temporary objects, because they never escape from
the inner loop.  The same remarks apply to the other two operations inside
the loop.

As a result, the code executed after the first iteration is close to optimal:
the intermediate values are stored as \lstinline{int} local variables, and the
multiplication, subtraction and \emph{less-than} comparison are mapped to a
single CLI opcode (\lstinline{mul}, \lstinline{sub} and \lstinline{clt},
respectively).

Moreover, we also wrote a program to calculate the $n_{th}$ Fibonacci number,
for which we can do the same reasoning as above.

Table XXX and figure XXX show the time spent to calculate the factorial of
various numbers, with and without the JIT.  Table XXX and figure XXX show the
same informations for the Fibonacci program.

\anto{Should we say that we get wrong results due to overflow but we don't care?}

As we can see, the code generated by the JIT is almost 500 times faster than
the non-jitted case, and it is only about 1.5 times slower than the same
algorithm written in C\#: the difference in speed it is probably due to both
the fact that the current CLI backend emits slightly non-optimal code and that
the underyling .NET JIT compiler is highly optimized to handle bytecode
generated by C\# compilers.

\subsection{Object-oriented features}

To measure how the JIT handles object-oriented features, we wrote a very
simple benchmark that involves attribute lookups and method calls.  We have an
\emph{accumulator} object, which has a field \lstinline{value} and a method
\lstinline{add}.  The method \lstinline{add} takes a parameter and adds it the
field \lstinline{value}.

XXX: update to the new version

Our benchmark accepts a paramter \lstinline{n}, create an \emph{accumulator},
and repeatedly calls \lstinline{add} on it, passing numbers from
\lstinline{n-1} to \lstinline{0}.

The JIT is able to completely remove the overhead of object creation,
attribute lookups and method calls, and the generated code results in a simple
loop doing additions in-place.

Table XXX and figure XXX show the time spent to run the benchmark with various
input arguments. Again, we can see that the jitted code is up to 500 times
faster than the interpreted one.
