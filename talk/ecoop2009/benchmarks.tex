\section{Benchmarks}

\cfbolz{I think this should go to the beginning of the description of the TLC as
it explains why it is written as it is written}

Despite being very simple and minimalistic, \lstinline{TLC} is a good
candidate as a language to run benchmarks, as it has some of the features that
makes most of current dynamic languages so slow:

\begin{itemize}

\item \textbf{Stack based VM}: this kind of VM requires all the operands to be
  on top of the evaluation stack.  As a consequence programs spend a lot of
  time pushing and popping values to/from the stack, or doing other stack
  related operations.  However, thanks to its simplicity this is still the
  most common and preferred way to implement VMs.

\item \textbf{Boxed integers}: integer objects are internally represented as
  an instance of the \lstinline{IntObj} class, whose field \lstinline{value}
  contains the real value.  By having boxed integers, common arithmetic
  operations are made very slow, because each time we want to load/store their
  value we need to go through an extra level of indirection.  Moreover, in
  case of a complex expression, it is necessary to create many temporary
  objects to hold intermediate results.

\item \textbf{Dynamic lookup}: attributes and methods are looked up at
  runtime, because there is no way to know in advance if and where an object
  have that particular attribute or method.
\end{itemize}

In the following sections, we will show some benchmarks that show how our
generated JIT can handle all the features above very well.

To measure the speedup we get with the JIT, we run each program three times:

\begin{enumerate}
\item By plain interpretation, without any jitting.
\item With the jit enabled: this run includes the time spent by doing the
  compilation itself, plus the time spent by running the produced code.
\item Again with the jit enabled, but this time the compilation has already
  been done, so we are actually measuring how good is the code we produced.
\end{enumerate}

The benchmarks have been run on machine XXX with hardware YYY etc. etc.

\subsection{Arithmetic operations}

To benchmark arithmetic operations between integers, we wrote a simple program
that computes the factorial of a given number.  The algorithm is
straightforward, and the loop contains only three operations: one
multiplication, one subtraction, and one comparison to check if we have
finished the job.

When doing plain interpretation, we need to create and destroy three temporary
objects at each iterations.  By contrast, the code generated by the JIT does
much better.  At the first iteration, the classes of the two operands of the
multiplication are promoted; then, the JIT compiler knows that both are
integers, so it can inline the code to compute the result.  Moreover, it can
\emph{virtualize} all the temporary objects, because they never escape from
the inner loop.  The same remarks apply to the other two operations inside
the loop.

As a result, the code executed after the first iteration is close to optimal:
the intermediate values are stored as \lstinline{int} local variables, and the
multiplication, subtraction and \emph{less-than} comparison are mapped to a
single CLI opcode (\lstinline{mul}, \lstinline{sub} and \lstinline{clt},
respectively).

Moreover, we also wrote a program to calculate the $n_{th}$ Fibonacci number,
for which we can do the same reasoning as above.

Table XXX and figure XXX show the time spent to calculate the factorial of
various numbers, with and without the JIT.  Table XXX and figure XXX show the
same informations for the Fibonacci program.

\anto{Should we say that we get wrong results due to overflow but we don't care?}

As we can see, the code generated by the JIT is almost 500 times faster than
the non-jitted case, and it is only about 1.5 times slower than the same
algorithm written in C\#, which can be considered the optimal goal.

\subsection{Object-oriented features}

To measure how the JIT handles object-oriented features, we wrote a very
simple benchmark that involves attribute lookups and method calls.  We have an
\emph{accumulator} object, which has a field \lstinline{value} and a method
\lstinline{add}.  The method \lstinline{add} takes a parameter and adds it the
field \lstinline{value}.

XXX: update to the new version

Our benchmark accepts a paramter \lstinline{n}, create an \emph{accumulator},
and repeatedly calls \lstinline{add} on it, passing numbers from
\lstinline{n-1} to \lstinline{0}.

The JIT is able to completely remove the overhead of object creation,
attribute lookups and method calls, and the generated code results in a simple
loop doing additions in-place.

Table XXX and figure XXX show the time spent to run the benchmark with various
input arguments. Again, we can see that the jitted code is up to 500 times
faster than the interpreted one.
