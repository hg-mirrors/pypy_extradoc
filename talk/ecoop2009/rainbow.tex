\section{Automatic Unboxing of Intermediate Results}
\label{sec:virtuals}

XXX the following section needs a rewriting to be much more high-level and to
compare more directly with classical escape analysis

\anto{Maybe we should talk about ``virtual instances'' and not structures,
  considering the context}

Interpreters for dynamic languages typically continuously allocate a lot of small
objects, for example due to boxing. This makes arithmetic operations extremely
inefficient. For this reason, we
implemented a way for the compiler to try to avoid memory allocations in the
residual code as long as possible. The idea is to try to keep new
run-time structures "exploded": instead of a single run-time object allocated on
the heap, the object is "virtualized" as a set
of fresh local variables, one per field. Only when the object can be accessed by from
somewhere else is it actually allocated on the heap. The effect of this is similar to that of
escape analysis \cite{XXX}, which also prevents allocations of objects that can
be proven to not escape a method or set of methods.

It is not always possible to keep structures virtual.  The main
situation in which it needs to be "forced" (i.e. actually allocated at
run-time) is when the pointer escapes to some non-virtual location like
a field of a real heap structure.  Virtual structures still avoid the run-time
 allocation of most short-lived objects, even in non-trivial situations.  

In addition to virtual structures, the compiler can also handle virtual
containers, namely lists and dictionaries \footnote{(R)Python's dictionaries
  are equivalent to .NET \lstinline{Hashtable}s}.  If the indexing operations
can be evaluated at compile-time (i.e., if the variables holding the indexes
are green), the compiler internally keeps track of the state of the container
and store the items as local variables.

Look again at figure \ref{fig:tlc-folded}: the list in the \lstinline{stack}
variable never escapes from the function.  Moreover, all the indexing
operations (either done explicitly or implicitly by \lstinline{append} and
\lstinline{pop}) are evaluable at compile-time.  Thus, the list is kept
\emph{virtual} and its elements are stored in variables $v_n$, where $n$
represents the index in the list.  Figure \ref{fig:tlc-folded-virtualized}
show how the resulting code looks like; to ease the reading, the state of the
\lstinline{stack} as kept by the compiler is shown in the comments.

\begin{figure}[h]
\begin{center}
\input{tlc-folded-virtualized.py}
\caption{The result of virtualizing the \lstinline{stack} list}
\label{fig:tlc-main}
\end{center}
\end{figure}

Even if not shown in the example, \lstinline{stack} is not the only
virtualized object.  In particular the two objects created by
\lstinline{IntObj(0)} are also virtualized, and their fields are stored as
local variables as well.  Virtualizion of instances is important not only
because it avoids the allocation of unneeded temporary objects, but also
because it makes possible to optimize method calls on them, as the JIT
compiler knows their exact type in advance.

XXX kill the rest?!‽
An interesting effect of virtual structures is that they play nicely with
promotion.  Indeed, before the interpreter can call the proper \texttt{add()}
function for integers, it must first determine that the two arguments
are indeed integer objects.  In the corresponding dispatch logic, we
have added two hints to promote the type of each of the two arguments.
This produces a compiler that has the following behavior: in the general
case, the expression \texttt{a+b} will generate two consecutive run-time
switches followed by the residual code of the proper version of
\texttt{add()}.  However, in \texttt{a+b+c}, the virtual structure representing
the intermediate value will contain a compile-time constant as type.
Promoting a compile-time constant is trivial – no run-time code is
generated.  The whole expression \texttt{a+b+c} thus only requires three
switches instead of four.  It is easy to see that even more switches can
be skipped in larger examples; typically, in a tight loop manipulating
only integers, all objects are virtual structures for the compiler and
the residual code is theoretically optimal – all type propagation and
boxing/unboxing occurs at compile-time.


\section{Promotion}
\label{sec:promotion}

In the sequel, we describe in more details one of the main new
techniques introduced in our approach, which we call \emph{promotion}.  In
short, it allows an arbitrary run-time (i.e. red) value to be turned into a
compile-time (i.e. green) value at any point in time.  Promotion is thus the central way by
which we make use of the fact that the JIT is running interleaved with actual
program execution. Each promotion point is explicitly defined with a hint that
must be put in the source code of the interpreter.

From a partial evaluation point of view, promotion is the converse of
the operation generally known as "lift" \cite{XXX}.  Lifting a value means
copying a variable whose binding time is compile-time into a variable
whose binding time is run-time – it corresponds to the compiler
"forgetting" a particular value that it knew about.  By contrast,
promotion is a way for the compiler to gain \emph{more} information about
the run-time execution of a program. Clearly, this requires
fine-grained feedback from run-time to compile-time, thus a
dynamic setting.

Promotion requires interleaving compile-time and run-time phases,
otherwise the compiler can only use information that is known ahead of
time. It is impossible in the "classical" approaches to partial
evaluation, in which the compiler always runs fully ahead of execution.
This is a problem in many realistic use cases.  For example, in an
interpreter for a dynamic language, there is mostly no information
that can be clearly and statically used by the compiler before any
code has run.

A very different point of view on promotion is as a generalization of techniques
that already exist in dynamic compilers as found in modern virtual machines for
object-oriented language.  In this context feedback
techniques are crucial for good results.  The main goal is to
optimize and reduce the overhead of dynamic dispatching and indirect
invocation.  This is achieved with variations on the technique of
polymorphic inline caches \cite{hoelzle_optimizing_1991}: the dynamic lookups are cached and
the corresponding generated machine code contains chains of
compare-and-jump instructions which are modified at run-time.  These
techniques also allow the gathering of information to direct inlining for even
better optimization results. 
\anto{What about this: While traditional PICs are only applied to indirect
  calls, promotion is a more general operation that can be applied to any kind
  of value, including instances of user-defined classes or integer numbers}

In the presence of promotion, dispatch optimization can usually be
reframed as a partial evaluation task.  Indeed, if the type of the
object being dispatched to is known at compile-time, the lookup can be
folded, and only a (possibly even inlined) direct call remains in the
generated code.  In the case where the type of the object is not known
at compile-time, it can first be read at run-time out of the object and
promoted to compile-time.  As we will see in the sequel, this produces
machine code very similar to that of polymorphic inline
caches.

The essential advantage of promotion is that it is no longer tied to the details of
the dispatch semantics of the language being interpreted, but applies in
more general situations.  Promotion is thus the central enabling
primitive to make partial evaluation a practical approach to language
independent dynamic compiler generation.

Promotion is invoked with the use of a hint as well:
\texttt{v2 = hint(v1, promote=True)}.
This hint is a \emph{local} request for \texttt{v2} to be green, without
requiring \texttt{v1} to be green.  Note that this amounts to copying
a red value into a green one, which is not possible in classical
approaches to partial evaluation. A slightly different hint can be used to
promote the \emph{class} of an instance. This is done with \lstinline{hint(v1,
promote_class=True)}. It does not have an effect on the bindings of any
variable.


\subsection{Implementing Promotion}

The implementation of promotion requires a tight coupling between
compile-time and run-time: a \emph{callback}, put in the generated code,
which can invoke the compiler again.  When the callback is actually
reached at run-time, and only then, the compiler resumes and uses the
knowledge of the actual run-time value to generate more code.

The new generated code is potentially different for each run-time value
seen.  This implies that the generated code needs to contain some sort
of updatable switch, or \emph{flexswitch}, which can pick the right code path based on the
run-time value.

Let us look again at the TLC example.  To ease the reading, figure
\ref{fig:tlc-main} showed a simplified version of TLC's main loop, which did
not include the hints.  The real implementation of the \lstinline{LT} opcode
is shown in figure \ref{fig:tlc-main-hints}.

\begin{figure}[h]
\begin{center}
\begin{lstlisting}[language=Python]
        elif opcode == LT:
            a, b = stack.pop(), stack.pop()
            hint(a, promote_class=True)
            hint(b, promote_class=True)
            stack.append(IntObj(b.lt(a)))
\end{lstlisting}
\caption{Usage of hints in TLC's main loop}
\label{fig:tlc-main-hints}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{lstlisting}[language=Python]
class IntObj(Obj):
    ...
    def lt(self, other): 
        return self.value < other.int_o()
    def int_o(self):
        return self.value
\end{lstlisting}
\caption{Excerpt of the \lstlisting{IntObj} class}
\label{fig:tlc-intobj}
\end{center}
\end{figure}

By promoting the class of \lstlisting{a} and \lstlisting{b}, we tell the JIT
compiler not to generate code until it knows the exact RPython class of both.
Figure \ref{fig:tlc-abs-promotion-1} shows the
code \footnote{\lstinline{switch} is not a legal (R)Python statement, it is
  used here only as a pseudocode example} generated while compiling the usual
\lstlisting{abs} function: note that, compared to figure
\ref{fig:tlc-folded-virtualized}, the code stops just before the calls
\lstlisting{b.lt(a)}.

\begin{figure}[h]
\begin{center}
\begin{lstlisting}[language=Python]
def interp_eval_abs(args):
    v0 = args[0]
    v1 = IntObj(0)
    a, b = v0, v1
    hint(a, promote_class=True) # no-op
    cls_a = a.__class__
    switch cls_a:
        default: 
            continue_compilation(jitstate, cls_a)
\end{lstlisting}
\caption{Promotion example 1}
\label{fig:tlc-abs-promotion-1}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\begin{lstlisting}[language=Python]
def interp_eval_abs(args):
    v0 = args[0]
    v1 = IntObj(0)
    a, b = v0, v1
    hint(a, promote_class=True) # no-op
    cls_a = a.__class__
    switch cls_a:
        IntObj:
            hint(b, promote_class=True)
            v0 = IntObj(b.value < a.value)
            ...
        default: 
            continue_compilation(jitstate, cls_a)
\end{lstlisting}
\caption{Promotion example 2}
\label{fig:tlc-abs-promotion-2}
\end{center}
\end{figure}

The first time the flexswitch is executed, the \lstinline{default} branch is
taken, and the special function \lstinline{continue_compilation} restarts the
JIT compiler, passing it the just-seen value of \lstinline{cls_a}.  The JIT
compiler generates new specialized code, and \emph{patches} the flexswitch to
add the new case, which is then executed.

If later an instance of \lstinline{IntObj} hits the flexswitch again, the
code is executed without needing of more calls to the JIT compiler.  On the
other hand, if the flexswitch is hit by an instance of some other class, the
\lstinline{default} branch will be selected again and the whole process will
restart.

Now, let us examine the content of the \lstinline{IntObj} case: first, there
is a hint to promote the class of \lstinline{b}.  Although in general
promotion is implemented through a flexswitch, in this case it is not needed
as \lstinline{b} holds a \emph{virtual instance}, whose class is already
known (as described in previous section).

Then, the compiler knows the exact class of \lstinline{b}, thus it can inline
the calls to \lstinline{lt}.  Moreover, inside \lstinline{lt} there is a
call to \lstinline{a.int_o()}, which is inlined as well for the very same
reason.

Moreover, as we saw in section \ref{sec:virtuals}, the \lstinline{IntObj}
instance can be virtualized, so that the subsequent \lstinline{BR_COND} opcode
can be compiled efficiently without needing any more flexswitch.
