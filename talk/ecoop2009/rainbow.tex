\section{Automatic Unboxing of Intermediate Results}
\label{sec:virtuals}

XXX the following section needs a rewriting to be much more high-level and to
compare more directly with classical escape analysis

Interpreters for dynamic languages typically continuously allocate a lot of small
objects, for example due to boxing. This makes arithmetic operations extremely
inefficient. For this reason, we
implemented a way for the compiler to try to avoid memory allocations in the
residual code as long as possible. The idea is to try to keep new
run-time structures "exploded": instead of a single run-time object allocated on
the heap, the object is "virtualized" as a set
of fresh local variables, one per field. Only when the object can be accessed by from
somewhere else is it actually allocated on the heap. The effect of this is similar to that of
escape analysis \cite{XXX}, which also prevents allocations of objects that can
be proven to not escape a method or set of methods.

It is not always possible to keep structures virtual.  The main
situation in which it needs to be "forced" (i.e. actually allocated at
run-time) is when the pointer escapes to some non-virtual location like
a field of a real heap structure.

Virtual structures still avoid the run-time allocation of most
short-lived objects, even in non-trivial situations.  The following
example shows a typical case.  The TLC interpreter implements application-level
integers as boxes – instances of an \texttt{IntObject} class with a single
field.


The PyPy Python interpreter
implements application-level integers as boxes – instances of a
\texttt{W\_IntObject} class with a single \texttt{intval} field.  Here is the
addition of two integers:

XXX needs to use TLC examples
\begin{verbatim}
    def add(w1, w2):            # w1, w2 are W_IntObject instances
        value1 = w1.intval
        value2 = w2.intval
        result = value1 + value2
        return W_IntObject(result)
\end{verbatim}

XXX kill the rest?!‽
When interpreting the bytecode for \texttt{a+b+c}, two calls to \texttt{add()} are
issued; the intermediate \texttt{W\_IntObject} instance is built by the first
call and thrown away after the second call.  By contrast, when the
interpreter is turned into a compiler, the construction of the
\texttt{W\_IntObject} object leads to a virtual structure whose \texttt{intval}
field directly references the register in which the run-time addition
put its result.  This location is read out of the virtual structure at
the beginning of the second \texttt{add()}, and the second run-time addition
directly operates on the same register.

An interesting effect of virtual structures is that they play nicely with
promotion.  Indeed, before the interpreter can call the proper \texttt{add()}
function for integers, it must first determine that the two arguments
are indeed integer objects.  In the corresponding dispatch logic, we
have added two hints to promote the type of each of the two arguments.
This produces a compiler that has the following behavior: in the general
case, the expression \texttt{a+b} will generate two consecutive run-time
switches followed by the residual code of the proper version of
\texttt{add()}.  However, in \texttt{a+b+c}, the virtual structure representing
the intermediate value will contain a compile-time constant as type.
Promoting a compile-time constant is trivial – no run-time code is
generated.  The whole expression \texttt{a+b+c} thus only requires three
switches instead of four.  It is easy to see that even more switches can
be skipped in larger examples; typically, in a tight loop manipulating
only integers, all objects are virtual structures for the compiler and
the residual code is theoretically optimal – all type propagation and
boxing/unboxing occurs at compile-time.


\section{Promotion}
\label{sec:promotion}

In the sequel, we describe in more details one of the main new
techniques introduced in our approach, which we call \emph{promotion}.  In
short, it allows an arbitrary run-time (i.e. red) value to be turned into a
compile-time (i.e. green) value at any point in time.  Promotion is thus the central way by
which we make use of the fact that the JIT is running interleaved with actual
program execution. Each promotion point is explicitly defined with a hint that
must be put in the source code of the interpreter.

From a partial evaluation point of view, promotion is the converse of
the operation generally known as "lift" \cite{XXX}.  Lifting a value means
copying a variable whose binding time is compile-time into a variable
whose binding time is run-time – it corresponds to the compiler
"forgetting" a particular value that it knew about.  By contrast,
promotion is a way for the compiler to gain \emph{more} information about
the run-time execution of a program. Clearly, this requires
fine-grained feedback from run-time to compile-time, thus a
dynamic setting.

Promotion requires interleaving compile-time and run-time phases,
otherwise the compiler can only use information that is known ahead of
time. It is impossible in the "classical" approaches to partial
evaluation, in which the compiler always runs fully ahead of execution.
This is a problem in many realistic use cases.  For example, in an
interpreter for a dynamic language, there is mostly no information
that can be clearly and statically used by the compiler before any
code has run.

A very different point of view on promotion is as a generalization of techniques
that already exist in dynamic compilers as found in modern virtual machines for
object-oriented language.  In this context feedback
techniques are crucial for good results.  The main goal is to
optimize and reduce the overhead of dynamic dispatching and indirect
invocation.  This is achieved with variations on the technique of
polymorphic inline caches \cite{hoelzle_optimizing_1991}: the dynamic lookups are cached and
the corresponding generated machine code contains chains of
compare-and-jump instructions which are modified at run-time.  These
techniques also allow the gathering of information to direct inlining for even
better optimization results.

In the presence of promotion, dispatch optimization can usually be
reframed as a partial evaluation task.  Indeed, if the type of the
object being dispatched to is known at compile-time, the lookup can be
folded, and only a (possibly even inlined) direct call remains in the
generated code.  In the case where the type of the object is not known
at compile-time, it can first be read at run-time out of the object and
promoted to compile-time.  As we will see in the sequel, this produces
machine code very similar to that of polymorphic inline
caches.

The essential advantage of promotion is that it is no longer tied to the details of
the dispatch semantics of the language being interpreted, but applies in
more general situations.  Promotion is thus the central enabling
primitive to make partial evaluation a practical approach to language
independent dynamic compiler generation.

\subsection{Implementing Promotion}

The implementation of promotion requires a tight coupling between
compile-time and run-time: a \emph{callback}, put in the generated code,
which can invoke the compiler again.  When the callback is actually
reached at run-time, and only then, the compiler resumes and uses the
knowledge of the actual run-time value to generate more code.

The new generated code is potentially different for each run-time value
seen.  This implies that the generated code needs to contain some sort
of updatable switch, which can pick the right code path based on the
run-time value.

\cfbolz{I think this example is confusing, it is unclear in which order things
happen how. I will try to come up with something different}.

\begin{itemize}
  XXX remove mention of rainbow interp. but this needs to be rewritten anyway
  \item (compile time): the rainbow interpreter produces machine code until it
    hits a promotion point; e.g.::

    \begin{lstlisting}[language=C]
        y = hint(x, promote=True)
        return y+10
    \end{lstlisting}

  \item (compile time): at this point, it generates special machine code that when
    reached calls the JIT compiler again; the JIT compilation stops::

    \begin{lstlisting}[language=C]
        switch(y) {
            default: compile_more(y);
        }
    \end{lstlisting}

  \item (runtime): the machine code is executed; when it reaches a promotion
    point, it executes the special machine code we described in the previous
    point; the JIT compiler is invoked again;

  \item (compile time): now we finally know the exact value of our red variable,
    and we can promote it to green; suppose that the value of 'y' is 32::

    \begin{lstlisting}[language=C]
        switch(y) {
            32: return 42;
            default: compile_more(y);
        }
    \end{lstlisting}

    Note that the operation "y+10" has been constant-folded into "42", as it
    was a green operation.

  \item (runtime) the execution restart from the point it stopped, until a new
    unhandled promotion point is reached.
\end{itemize}

\subsection{Promotion as Applied to the TLC}

XXX maybe a tlc example can be used for the example above?


