\section{The Rainbow Interpreter}

The JIT compiler is implemented by running the "Rainbow interpreter", whose
output is a compiled function.

Variables and opcodes are colored:

  - green values are known at compile time, and green operations are executed
    when interpreting the bytecode;

  - red values are not known until runtime; the result of a red operation is a
    piece of machine code that will compute the result at runtime.

The Rainbow bytecode is produced at translation time, when the JIT compiler is
generated.

\cfbolz{XXX I think we should be very careful with the rainbow interp. it is a
total implementation-detail and we should only describe it as little as
possible}

\subsection{Example of Rainbow bytecode and execution}

TODO

\section{Promotion}

In the sequel, we describe in more details one of the main new
techniques introduced in our approach, which we call \emph{promotion}.  In
short, it allows an arbitrary run-time value to be turned into a
compile-time value at any point in time.  Promotion is thus the central way by
which we make use of the fact that the JIT is running interleaved with actual
program execution. Each promotion point is explicitly defined with a hint that
must be put in the source code of the interpreter.

From a partial evaluation point of view, promotion is the converse of
the operation generally known as "lift" \cite{XXX}.  Lifting a value means
copying a variable whose binding time is compile-time into a variable
whose binding time is run-time – it corresponds to the compiler
"forgetting" a particular value that it knew about.  By contrast,
promotion is a way for the compiler to gain \emph{more} information about
the run-time execution of a program. Clearly, this requires
fine-grained feedback from run-time to compile-time, thus a
dynamic setting.

Promotion requires interleaving compile-time and run-time phases,
otherwise the compiler can only use information that is known ahead of
time. It is impossible in the "classical" approaches to partial
evaluation, in which the compiler always runs fully ahead of execution
This is a problem in many large use cases.  For example, in an
interpreter for a dynamic language, there is mostly no information
that can be clearly and statically used by the compiler before any
code has run.

A very different point of view on promotion is as a generalization of
techniques that already exist in dynamic compilers as found in modern
object-oriented language virtual machines.  In this context feedback
techniques are crucial for good results.  The main goal is to
optimize and reduce the overhead of dynamic dispatching and indirect
invocation.  This is achieved with variations on the technique of
polymorphic inline caches \cite{XXX}: the dynamic lookups are cached and
the corresponding generated machine code contains chains of
compare-and-jump instructions which are modified at run-time.  These
techniques also allow the gathering of information to direct inlining for even
better optimization results.

In the presence of promotion, dispatch optimization can usually be
reframed as a partial evaluation task.  Indeed, if the type of the
object being dispatched to is known at compile-time, the lookup can be
folded, and only a (possibly inlined) direct call remains in the
generated code.  In the case where the type of the object is not known
at compile-time, it can first be read at run-time out of the object and
promoted to compile-time.  As we will see in the sequel, this produces
very similar machine code \footnote{This can also be seen as a generalization of
a partial evaluation transformation called "The Trick" (see e.g. \cite{XXX}),
which again produces similar code but which is only applicable for finite sets
of values.}.

The essential advantage is that it is no longer tied to the details of
the dispatch semantics of the language being interpreted, but applies in
more general situations.  Promotion is thus the central enabling
primitive to make partial evaluation a practical approach to language
independent dynamic compiler generation.

\subsection{Promotion as Applied to the TLC}

XXX

\subsection{Promotion in Practise}

There are values that, if known at compile time, allow the JIT compiler to
produce very efficient code.  Unfortunately, these values are tipically red,
e.g. the exact type of a variable.

"Promotion" is a particular operation that convert a red value into a green
value; i.e., after the promotion of a variable, the JIT compiler knows its
value at compile time. Since the value of a red variable is not known until
runtime, we need to postpone the compilation phase after the runtime phase.

This is done by continuously intermixing compile time and runtime; a promotion
is implemented in this way:

\begin{itemize}
  \item (compile time): the rainbow interpreter produces machine code until it
    hits a promotion point; e.g.::

    \begin{lstlisting}[language=C]
        y = hint(x, promote=True)
        return y+10
    \end{lstlisting}

  \item (compile time): at this point, it generates special machine code that when
    reached calls the JIT compiler again; the JIT compilation stops::

    \begin{lstlisting}[language=C]
        switch(y) {
            default: compile_more(y);
        }
    \end{lstlisting}

  \item (runtime): the machine code is executed; when it reaches a promotion
    point, it executes the special machine code we described in the previous
    point; the JIT compiler is invoked again;

  \item (compile time): now we finally know the exact value of our red variable,
    and we can promote it to green; suppose that the value of 'y' is 32::

    \begin{lstlisting}[language=C]
        switch(y) {
            32: return 42;
            default: compile_more(y);
        }
    \end{lstlisting}

    Note that the operation "y+10" has been constant-folded into "42", as it
    was a green operation.

  \item (runtime) the execution restart from the point it stopped, until a new
    unhandled promotion point is reached.
\end{itemize}

\section{Automatic Unboxing of Intermediate Results}

XXX the following section needs a rewriting to be much more high-level and to
compare more directly with classical escape analysis

Interpreters for dynamic languages typically allocate a lot of small
objects, for example due to boxing.  For this reason, we
implemented a way for the compiler to generate residual memory
allocations as lazily as possible.  The idea is to try to keep new
run-time structures "exploded": instead of a single run-time pointer to
a heap-allocated data structure, the structure is "virtualized" as a set
of fresh variables, one per field.  In the compiler, the variable that
would normally contain the pointer to the structure gets instead a
content that is neither a run-time value nor a compile-time constant,
but a special \emph{virtual structure} – a compile-time data structure that
recursively contains new variables, each of which can again store a
run-time, a compile-time, or a virtual structure value.

This approach is based on the fact that the "run-time values" carried
around by the compiler really represent run-time locations – the name of
a CPU register or a position in the machine stack frame.  This is the
case for both regular variables and the fields of virtual structures.
It means that the compilation of a \texttt{getfield} or \texttt{setfield}
operation performed on a virtual structure simply loads or stores such a
location reference into the virtual structure; the actual value is not
copied around at run-time.

It is not always possible to keep structures virtual.  The main
situation in which it needs to be "forced" (i.e. actually allocated at
run-time) is when the pointer escapes to some non-virtual location like
a field of a real heap structure.

Virtual structures still avoid the run-time allocation of most
short-lived objects, even in non-trivial situations.  The following
example shows a typical case.  Consider the Python expression \texttt{a+b+c}.
Assume that \texttt{a} contains an integer.  The PyPy Python interpreter
implements application-level integers as boxes – instances of a
\texttt{W\_IntObject} class with a single \texttt{intval} field.  Here is the
addition of two integers:

XXX needs to use TLC examples
\begin{verbatim}
    def add(w1, w2):            # w1, w2 are W_IntObject instances
        value1 = w1.intval
        value2 = w2.intval
        result = value1 + value2
        return W_IntObject(result)
\end{verbatim}

When interpreting the bytecode for \texttt{a+b+c}, two calls to \texttt{add()} are
issued; the intermediate \texttt{W\_IntObject} instance is built by the first
call and thrown away after the second call.  By contrast, when the
interpreter is turned into a compiler, the construction of the
\texttt{W\_IntObject} object leads to a virtual structure whose \texttt{intval}
field directly references the register in which the run-time addition
put its result.  This location is read out of the virtual structure at
the beginning of the second \texttt{add()}, and the second run-time addition
directly operates on the same register.

An interesting effect of virtual structures is that they play nicely with
promotion.  Indeed, before the interpreter can call the proper \texttt{add()}
function for integers, it must first determine that the two arguments
are indeed integer objects.  In the corresponding dispatch logic, we
have added two hints to promote the type of each of the two arguments.
This produces a compiler that has the following behavior: in the general
case, the expression \texttt{a+b} will generate two consecutive run-time
switches followed by the residual code of the proper version of
\texttt{add()}.  However, in \texttt{a+b+c}, the virtual structure representing
the intermediate value will contain a compile-time constant as type.
Promoting a compile-time constant is trivial – no run-time code is
generated.  The whole expression \texttt{a+b+c} thus only requires three
switches instead of four.  It is easy to see that even more switches can
be skipped in larger examples; typically, in a tight loop manipulating
only integers, all objects are virtual structures for the compiler and
the residual code is theoretically optimal – all type propagation and
boxing/unboxing occurs at compile-time.
