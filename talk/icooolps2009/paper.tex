\documentclass{acm_proc_article-sp}

\usepackage{ifthen}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{ulem}
\usepackage[utf8]{inputenc}

\newboolean{showcomments}
\setboolean{showcomments}{true}
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id: main.tex 19055 2008-06-05 11:20:31Z cfbolz $-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }

\newcommand\cfbolz[1]{\nb{CFB}{#1}}
\newcommand\anto[1]{\nb{ANTO}{#1}}
\newcommand\arigo[1]{\nb{AR}{#1}}
\newcommand\fijal[1]{\nb{FIJAL}{#1}}
\newcommand{\commentout}[1]{}

\normalem

\let\oldcite=\cite

\renewcommand\cite[1]{\ifthenelse{\equal{#1}{XXX}}{[citation~needed]}{\oldcite{#1}}}

\begin{document}

\title{Tracing the Meta-Level: PyPy's Tracing JIT Compiler}

\numberofauthors{3}
\author{
\alignauthor Carl Friedrich Bolz\\
       \affaddr{Heinrich-Heine-Universität Düsseldorf}\\
       \affaddr{Softwaretechnik und Programmiersprachen}\\
       \affaddr{Institut für Informatik}\\ 
       \affaddr{Universitätsstra{\ss}e 1}\\
       \affaddr{D-40225 Düsseldorf}\\
       \affaddr{Deutschland}\\
       \email{cfbolz@gmx.de}
\alignauthor Antonio Cuni\\
       \affaddr{University of Genova}\\
       \affaddr{DISI}\\
       \affaddr{Italy}\\
       \email{cuni@disi.unige.it}
\alignauthor Armin Rigo\\
       \email{arigo@tunes.org}
}
\maketitle


%\category{D.3.4}{Programming Languages}{Processors}[code generation,
%interpreters, run-time environments]
%\category{F.3.2}{Logics and Meanings of Programs}{Semantics of Programming
%Languages}[program analysis]

\begin{abstract}
In this paper we describe the ongoing research in the PyPy project to write a
JIT compiler that is automatically adapted to various languages, given an
interpreter for that language. This is achieved with the help of a slightly
adapted tracing JIT compiler in combination with some hints by the author of the
interpreter.  XXX

\end{abstract}

XXX write somewhere that one problem of using tracing JITs for dynamic languages
is that dynamic languages have very complex bytecodes


\section{Introduction}

Dynamic languages, rise in popularity, bla bla XXX

One of the often-cited drawbacks of dynamic languages is the performance
penalties they impose. Typically they are slower than statically typed languages
\cite{XXX}. Even though there has been a lot of research into improving the
performance of dynamic languages \cite{XXX}, those techniques are not as widely
used as one would expect. Many dynamic language implementations use completely
straightforward bytecode-interpreters without any advanced implementation
techniques like just-in-time compilation. There are a number of reasons for
this. Most of them boil down to the inherent complexities of using compilation.
Interpreters are simple to implement, understand, extend and port whereas writing a
just-in-time compiler is an error-prone task that is made even harder by the
dynamic features of a language.

A recent approach to getting better performance for dynamic languages is that of
tracing JIT compilers \cite{XXX}. Writing a tracing JIT compiler is relatively
simple, because it can be added to an existing interpreter for a language,
because the interpreter takes over some of the functionality of the compiler and
the machine code generation part can be simplified.
\anto{XXX small english issue: I don't like the two consecutive ``because'',
  I'm not even sure what is the right way to interpret the sentence}

The PyPy project is trying to find approaches to generally ease the
implementation of dynamic languages. It started as a Python implementation in
Python, but has now extended its goals to be generally useful for implementing
other dynamic languages as well. The general approach is to implement an
interpreter for the language in a subset of Python. This subset is chosen in
such a way that programs in it can be compiled into various target environments,
such as C/Posix, the CLI or the JVM. The PyPy project is described in more
details in Section \ref{sect:pypy}.

In this paper we discuss ongoing work in the PyPy project to improve the
performance of interpreters written with the help of the PyPy toolchain. The
approach is that of a tracing JIT compiler. Opposed to the tracing JITs for dynamic
languages that exist so far, PyPy's tracing JIT operates "one level down",
e.g. traces the execution of the interpreter, as opposed to the execution
of the user program. The fact that the program the tracing JIT compiles is
in our case always an interpreter brings its own set of problems. We describe
tracing JITs and their application to interpreters in Section
\ref{sect:tracing}.  By this approach we hope to get a JIT compiler that can be
applied to a variety of dynamic languages, given an interpreter for them. The
process is not completely automatic but needs a small number of hints from the
interpreter author, to help the tracing JIT. The details of how the process
integrates into the rest of PyPy will be explained in Section
\ref{sect:implementation}. This work is not finished, but already produces some
promising results, which we will discuss in Section \ref{sect:evaluation}.

The contributions of this paper are:
\begin{itemize}
\item Techniques for improving the generated code when applying a tracing JIT to
an interpreter
\item 
\end{itemize}


%- dynamic languages important
%- notoriously difficult to achieve good performance
%- even though the techniques exist since a while, not many implementations
%  actually use them
%    - hard to get all corner-cases right
%    - languages evolve
%    - modern dynamic languages are large
%    - open source/research communities don't have that many resources
%
%- PyPy project: trying find approaches to ease the implementation of dynamic
%languages
%- explore general ways to improve on the speed of dynamic languages with reduced
%effort
%- approach: write a tracing JIT that is applicable to many different languages,
%by tracing "one level done"
%- needs some hints by the interpreter-writer + slightly different optimizations
%- paper will describe the problems of applying a tracing jit to an interpreter
%- different integration needed than a typical tracing jit


\section{The PyPy Project}
\label{sect:pypy}

The PyPy project\footnote{http://codespeak.net/pypy}
\cite{rigo_pypys_2006,carl_friedrich_bolz_to_2007} was started to implement a
new Python interpreter in Python but has now extended its goals to be an
environment where flexible implementation of dynamic languages can be written.
To implement a dynamic language with PyPy, an interpreter for that language has
to be written in RPython \cite{AACM-DLS07}. RPython ("Restricted Python") is a subset of Python
chosen in such a way that type inference can be performed on it. The language
interpreter can then be translated with the help of PyPy into various target
environments, such as C/Posix, the CLI and the JVM. This is done by a component
of PyPy called the \emph{translation toolchain}.

By writing VMs in a high-level language, we keep the implementation of the
language free of low-level details such as memory management strategy,
threading model or object layout.  These features are automatically added
during the translation process. The process starts by performing control flow
graph construction and type inferences, then followed by a series of steps, each step
transforming the intermediate representation of the program produced by the
previous one until we get the final executable.  The first transformation step
makes details of the Python object model explicit in the intermediate
representation, later steps introducing garbage collection and other low-level
details. As we will see later, this internal representation of the program is
also used as an input for the tracing JIT.


%- original goal: Python interpreter in Python
%- general way to write flexible VMs for dynamic languages
%- interpreter written in RPython, subset of Python to allow type inference
%- translation toolchain
%   - naive forward-propagation type inference
%   - lowering of abstractions
%   - lltype system, monomorphic C-level operations
%   - type system: primitives, pointers to structs and arrays
%   - still assumes presence of GC
%   - can be interpreted in various ways

\section{Tracing JIT Compilers}
\label{sect:tracing}

\arigo{We should not start from scratch and insert as little details what
differs in our approach when compared to others; instead we should give a
higher-level overview and then focus on these details, and a couple of
references for more info about the "common" part.
%
In general there are many things that are never said at all.
I think the introduction should really be written from the point of view of
someone that has read already some papers for JavaScript.}


Tracing JITs are an idea initially explored by the Dynamo project
\cite{bala_dynamo:transparent_2000} in the context of dynamic optimization of
machine code at runtime. The techniques were then successfully applied to Java
VMs \cite{gal_hotpathvm:effective_2006}. It also turned out that they are a
relatively simple way to implement a JIT compiler for a dynamic language
\cite{mason_chang_efficient_2007}. The technique is now
being used by both Mozilla's TraceMonkey JavaScript VM \cite{XXX} and Adobe's
Tamarin ActionScript VM \cite{XXX}.

Tracing JITs are built on the following basic assumptions:

\begin{itemize}
 \item programs spend most of their runtime in loops
 \item several iterations of the same loop are likely to take similar code paths
\end{itemize}

The basic approach of a tracing JIT is to only generate machine code for the hot
code paths of commonly executed loops and to interpret the rest of the program.
The code for those common loops however should be highly optimized, including
aggressive inlining.

Typically, programs executed by a tracing VMs goes through various phases:

\begin{itemize}
\item Interpretation/profiling
\item Tracing
\item Code generation
\item Execution of the generated code
\end{itemize}

The \emph{code generation} phase takes as input the trace generated during
\emph{tracing}.

At first, when the program starts, everything is interpreted.
The interpreter does a bit of lightweight profiling to figure out which loops
are run often. This lightweight profiling is usually done by having a counter on
each backward jump instruction that counts how often this particular backward jump
was executed. Since loops need a backward jump somewhere, this method finds
loops in the user program.

When a hot loop is identified, the interpreter enters a
special mode, called \emph{tracing mode}. During tracing, the interpreter
records a history of all the operations it executes.

Such a history is called a \emph{trace}: it is a sequential list of
operations, together with their actual operands and results.  By examining the
trace, it is possible to produce highly efficient machine code by emitting
only the operations needed.  Being sequential, the trace represents only one
of the many possible paths through the code. To ensure correctness, the trace
contains a \emph{guard} at every possible point where the path could have
followed another direction, for example conditions or indirect/virtual
calls.  When emitting the machine code, every guard is turned into a quick check
to guarantee that the path we are executing is still valid.  If a guard fails,
we immediately quit from the machine code and continue the execution by falling
ways.  

During tracing, the trace is repeatedly
checked whether the interpreter is at a position in the program that it had seen
earlier in the trace. If this happens, the trace recorded corresponds to a loop
in the interpreted program that the tracing interpreter is running. At this point, this loop
is turned into machine code by taking the trace and making machine code versions
of all the operations in it. The machine code can then be immediately executed,
starting from the second iteration of the loop,
as it represents exactly the loop that was being interpreted so far.

This process assumes that the path through the loop that was traced is a
"typical" example of possible paths (which is statistically likely). Of course
it is possible that later another path through the loop is taken, therefore the
machine code will contain \emph{guards}, which check that the path is still the same.
If a guard fails during execution of the machine code, the machine code is left
and execution falls back to using interpretation (there are more complex
mechanisms in place to still produce more code for the cases of guard failures,
but they are of no importance for this paper).

It is important to understand when the tracer considers a loop in the trace to
be closed. This happens when the \emph{position key} is the same as at an earlier
point. The position key describes the position of the execution of the program,
e.g. usually contains things like the function currently being executed and the
program counter position of the tracing interpreter. The tracing interpreter
does not need to check all the time whether the position key already occurred
earlier, but only at instructions that are able to change the position key
to an earlier value, e.g. a backward branch instruction. Note that this is
already the second place where backward branches are treated specially: during
interpretation they are the place where the profiling is performed and where
tracing is started or already existing assembler code entered; during tracing
they are the place where the check for a closed loop is performed.

\fijal{RPython tracer operates on a level which is slightly higher level than
of the underlaying C. To be more precise: it operates on exception-transformed,
non-gc-transformed graphs with an extra bit of abstraction inversion that
finds out operations on loops and dicts, not sure how to express this}

Let's look at a small example. Take the following (slightly contrived) RPython
code:
\begin{verbatim}
def f(a, b):
    if b % 46 == 41:
        return a - b
    else:
        return a + b
def strange_sum(n):
    result = 0
    while n >= 0:
        result = f(result, n)
        n -= 1
    return result
\end{verbatim}

To trace this, a bytecode form of these functions needs to be introduced that
the tracer understands. The tracer interprets a bytecode that is an encoding of
the intermediate representation of PyPy's translation toolchain after type
inference has been performed and Python-specifics have been made explicit. At
first those functions will be interpreted, but after a while, profiling shows
that the \texttt{while} loop in \texttt{strange\_sum} is executed often.  The
tracing JIT will then start to trace the execution of that loop.  The trace would
look as follows:
\begin{verbatim}
loop_header(result0, n0)
i0 = int_mod(n0, Const(46))
i1 = int_eq(i0, Const(41))
guard_false(i1)
result1 = int_add(result0, n0)
n1 = int_sub(n0, Const(1))
i2 = int_ge(n1, Const(0))
guard_true(i2)
jump(result1, n1)
\end{verbatim}

The operations in this sequence are operations of the mentioned intermediate
representation (e.g. note that the generic modulo and equality operations in the
function above have been recognized to always work on integers and are thus
rendered as \texttt{int\_mod} and \texttt{int\_eq}). The trace contains all the
operations that were executed, is in SSA-form \cite{XXX} and ends with a jump
to its own beginning, forming an endless loop that can only be left via a guard
failure. The call to \texttt{f} was inlined into the trace. \sout{Of the condition in
\texttt{f} the much more common \texttt{else} case was traced. The other case is
implemented via a guard failure. This trace can then be turned into machine code
and executed.}
\anto{
Note that the trace contains only the hot \texttt{else} case of the
\texttt{if} test in \texttt{f}, while the other branch is implemented via a
guard failure. This trace can then be turned into machine code and executed.
}

%- general introduction to tracing
%- assumptions
%- mixed-mode execution environment: interpretation, tracing, compilation,
%  running native code
%- write why tracing jits are particularly well suited for dynamic languages

\subsection{Applying a Tracing JIT to an Interpreter}

The tracing JIT of the PyPy project is atypical in that it is not applied to the
user program, but to the interpreter running the user program. In this section
we will explore what problems this brings, and how to solve them (at least
partially). This means that there are two interpreters involved, and we need
terminology to distinguish them. On the one hand, there is the interpreter that
the tracing JIT uses to perform tracing. This we will call the \emph{tracing
interpreter}. On the other hand, there is the interpreter that is running the
users programs, which we will call the \emph{language interpreter}. In the
following, we will assume that the language interpreter is bytecode-based. The
program that the language interpreter executes we will call the \emph{user
program} (from the point of view of a VM author, the "user" is a programmer
using the VM).

Similarly, we need to distinguish loops at two different levels:
\emph{interpreter loops} are loops \emph{inside} the language interpreter. On
the other hand, \emph{user loops} are loops in the user program.

\fijal{I find following paragraph out of scope and completely confusing, we
should instead simply state that we unroll the loop, how we do that and
why we do that. Completely ignore aspect of an interpreter loop I suppose,
because everything previously keeps talking about can\_enter\_jit that closes
loop being available at jump back bytecodes}
A tracing JIT compiler finds the hot loops of the program it is compiling. In
our case, this program is the language interpreter. The most important hot loop
of the language interpreter is its bytecode dispatch loop (for many simple
interpreters it is also the only hot loops).  Tracing one iteration of this
loop means that
the recorded trace corresponds to execution of one opcode. This means that the
assumption that the tracing JIT makes -- that several iterations of a hot loop
take the same or similar code paths -- is just wrong in this case. It is very
unlikely that the same particular opcode is executed many times in a row.

\begin{figure}
\input{code/tlr-paper.py}
\caption{A very simple bytecode interpreter with registers and an accumulator.}
\label{fig:tlr-basic}
\end{figure}

\begin{figure}
\begin{verbatim}
    MOV_A_R     0   # i = a
    MOV_A_R     1   # copy of 'a'
    
    # 4:
    MOV_R_A     0   # i--
    DECR_A
    MOV_A_R     0    

    MOV_R_A     2   # res += a
    ADD_R_TO_A  1
    MOV_A_R     2
    
    MOV_R_A     0   # if i!=0: goto 4
    JUMP_IF_A   4

    MOV_R_A     2   # return res
    RETURN_A
\end{verbatim}
\caption{Example bytecode: Compute the square of the accumulator}
\label{fig:square}
\end{figure}

\fijal{This paragraph should go away as well}
Let's look at an example. Figure \ref{fig:tlr-basic} shows the code of a very
simple bytecode interpreter with 256 registers and an accumulator. The
\texttt{bytecode} argument is a string of bytes and all register and the
accumulator are integers. A simple program for this interpreter that computes
the square of the accumulator is shown in Figure \ref{fig:square}. If the
tracing interpreter traces the execution of the \texttt{DECR\_A} opcode (whose
integer value is 7), the trace would look as in Figure \ref{fig:trace-normal}.
Because of the guard on \texttt{opcode0}, the code compiled from this trace will
be useful only when executing a long series of \texttt{DECR\_A} opcodes. For all
the other operations the guard will fail, which will mean that performance is
probably not improved at all.

\begin{figure}
\input{code/normal-tracing.txt}
\caption{Trace when executing the \texttt{DECR\_A} opcode}
\label{fig:trace-normal}
\end{figure}

To improve this situation, the tracing JIT could trace the execution of several
opcodes, thus effectively unrolling the bytecode dispatch loop. Ideally, the
bytecode dispatch loop should be unrolled exactly so much, that the unrolled version
corresponds to \emph{user loop}. User loops
occur when the program counter of the language interpreter has the
same value several times. This program counter is typically stored in one or several
variables in the language interpreter, for example the bytecode object of the
currently executed function of the user program and the position of the current
bytecode within that.  In the example above, the program counter is represented by 
the \texttt{bytecode} and \texttt{pair} variables.

Since the tracing JIT cannot know which parts of the language interpreter are
the program counter, the author of the language interpreter needs to mark the
relevant variables of the language interpreter with the help of a \emph{hint}.
The tracing interpreter will then effectively add the values of these variables
to the position key. This means that the loop will only be considered to be
closed if these variables that are making up program counter at the language
interpreter level are the same a second time.  Loops found in this way are, by
definition, user loops.

The program counter of the language interpreter can only be the same a
second time after an instruction in the user program sets it to an earlier
value. This happens only at backward jumps in the language interpreter. That
means that the tracing interpreter needs to check for a closed loop only when it
encounters a backward jump in the language interpreter. Again the tracing JIT
cannot known which part of the language interpreter implements backward jumps,
so it needs to be told with the help of a hint by the author of the language
interpreter.

\fijal{This is wrong. Without virtuals there is also at most one assembler
loop per user loop. If it has more branches, we enter the loop as usual and
then we create a bridge for a new situation}
The condition for reusing already existing machine code needs to be adapted to
this new situation. In a classical tracing JIT there is at most one piece of
assembler code per loop of the jitted program, which in our case is the language
interpreter. When applying the tracing JIT to the language interpreter as
described so far, \emph{all} pieces of assembler code correspond to the bytecode
dispatch loop of the language interpreter. They correspond to different
unrollings and paths of that loop though. To figure out which of them to use
when trying to enter assembler code again, the program counter of the language
interpreter needs to be checked. If it corresponds to the position key of one of
the pieces of assembler code, then this assembler code can be entered. This
check again only needs to be performed at the backward branches of the language
interpreter.

The language interpreter uses a similar techniques to detect \emph{hot user
loops}: the profiling is done at the backward branches of the user program,
using one counter per seen program counter of the language interpreter.

\begin{figure}
\input{code/tlr-paper-full.py}
\caption{Simple bytecode interpreter with hints applied}
\label{fig:tlr-full}
\end{figure}

\fijal{Stopped reading at that point}
Let's look at which hints would need to be applied to the example interpreter
from Figure \ref{fig:tlr-basic}. The basic thing needed to apply hints is a
subclass of \texttt{JitDriver} that lists all the variables of the bytecode
loop. The variables are classified into two groups, red variables and green
variables. The green variables are those that the tracing JIT should consider to
be part of the program counter of the language interpreter. In the case of the
example, the \texttt{pc} variable is obviously part of the program counter.
However, the \texttt{bytecode} variable is also counted as green, since the
\texttt{pc} variable is meaningless without the knowledge of which bytecode
string is currently being interpreted. All other variables are red.

In addition to the classification of the variables, there are two methods of
\texttt{JitDriver} that need to be called. Both of them get as arguments the
current values of the variables listed in the definition of the driver. The
first one is \texttt{jit\_merge\_point} which needs to be put at the beginning
of the body of the bytecode dispatch loop. The other, more interesting one, is
\texttt{can\_enter\_jit}. This method needs to be called at the end of any
instruction that can set the program counter of the language interpreter to an
earlier value. For the example this is only the \texttt{JUMP\_IF\_A}
instruction, and only if it is actually a backward jump. The place where this
method is called is where the language interpreter performs profiling to decide
when to start tracing. It is also the place where the tracing JIT checks
whether a loop is closed. This is considered to be the case when the values of
the "green" variables are the same as at an earlier call to the
\texttt{can\_enter\_jit} method.

For the small example the hints look like a lot of work. However, the amount of
hints is essentially constant no matter how large the interpreter is, which
makes it seem less significant for larger interpreters.

When executing the Square function of Figure \ref{fig:square}, the profiling
will identify the loop in the square function to be hot, and start tracing. It
traces the execution of the interpreter running the loop of the square function
for one iteration, thus unrolling the interpreter loop of the example
interpreter eight times. The resulting trace can be seen in Figure 
\ref{fig:trace-no-green-folding}.

\begin{figure}
\input{code/no-green-folding.txt}
\caption{Trace when executing the Square function of Figure \ref{fig:square},
with the corresponding bytecodes as comments.}
\label{fig:trace-no-green-folding}
\end{figure}

XXX summarize at which points the tracing interpreter needed changing
XXX all changes only to the position key and when to enter/leave the tracer!
XXX tracing remains essentially the same

\subsection{Improving the Result}

The critical problem of tracing the execution of just one opcode has been
solved, the loop corresponds exactly to the loop in the square function.
However, the resulting trace is a bit too long. Most of its operations are not
actually doing any computation that is part of the square function. Instead,
they manipulate the data structures of the language interpreter. While this is
to be expected, given that the tracing interpreter looks at the execution of the
language interpreter, it would still be nicer if some of these operations could
be removed.

The simple insight how to greatly improve the situation is that most of the
operations in the trace are actually concerned with manipulating the
bytecode and the program counter. Those are stored in variables that are part of
the position key (they are "green"), that means that the tracer checks that they
are some fixed value at the beginning of the loop. In the example the check
would be that the \texttt{bytecode} variable is the bytecode string
corresponding to the square function and that the \texttt{pc} variable is
\texttt{4}. Therefore it is possible to constant-fold computations on them away,
as long as the operations are side-effect free. Since strings are immutable in
Python, it is possible to constant-fold the \texttt{strgetitem} operation. The
\texttt{int\_add} operations can be folded anyway.

With this optimization enabled, the trace looks as in Figure
\ref{fig:trace-full}. Now a lot of the language interpreter is actually gone
from the trace and what is left corresponds very closely to the loop of the
square function. The only vestige of the language interpreter is the fact that
the register list is still used to store the state of the computation. This
could be removed by some other optimization, but is maybe not really all that
bad anyway (in fact we have an experimental optimization that does exactly that,
but it is not finished).

\anto{XXX I propose to show also the trace with the malloc removal enabled, as it
  is much nicer to see. Maybe we can say that the experimental optimization we
  are working on would generate this and that} \cfbolz{This example is not about
  mallocs! There are no allocations in the loop. The fix would be to use
  maciek's lazy list stuff (or whatever it's called) which is disabled at the
  moment}

\begin{figure}
\input{code/full.txt}
\caption{Trace when executing the Square function of Figure \ref{fig:square},
with the corresponding opcodes as comments. The constant-folding of operations
on green variables is enabled.}
\label{fig:trace-full}
\end{figure}

Once we get this highly optimized trace, we can pass it to the \emph{JIT
backend}, which generates the correspondent machine code.

%- problem: typical bytecode loops don't follow the general assumption of tracing
%- needs to unroll bytecode loop
%    - how often to unroll
%    - when to start tracing?
%    - unroll exactly so that unrolled loop corresponds to loop of the user
%      program
%- how to improve matters: introducing merge keys
%- constant-folding of operations on green things
%    - similarities to BTA of partial evaluation


\section{Implementation Issues}
\label{sect:implementation}

In this section we will describe some of the practical issues when
implementing the scheme described in the last section in PyPy. In particular
we will describe some of the problems of integrating the various parts with
each other.

\anto{XXX: We shoud clarify the distinction between translation/compilation
  somewhere in the introduction}

The first integration problem is how to \emph{not} integrate the tracing JIT at
all. It should be possible to choose when the language interpreter is translated to C
whether the JIT should be built in or not. If the JIT is not enabled, all the
hints that are possibly in the interpreter source are just ignored by the
translation process. In this way, the result of the translation is identical to
as if no hints were present in the interpreter at all.

If the JIT is enabled, things are more interesting. At the moment the JIT can
only be enabled when translating the interpreter to C, but we hope to lift that
restriction in the future. A classical tracing JIT will
interpret the program it is running until a common loop is identified, at which
point tracing and ultimately assembler generation starts. The tracing JIT in
PyPy is operating on the language interpreter, which is written in RPython. But
RPython programs are statically translatable to C anyway. This means that interpreting the
language interpreter before a common loop is found is clearly not desirable,
since the overhead of this double-interpretation would be significantly too big
to be practical.

What is done instead is that the language interpreter keeps running as a C
program, until a common loop in the user program is found. To identify loops the
C version of the language interpreter is generated in such a way that at the
place that corresponds to the \texttt{can\_enter\_jit} hint profiling is
performed using the program counter of the language interpreter. Apart from this
bit of profiling, the language interpreter behaves in just the same way as
without a JIT.

When a hot user loop is identified, tracing is started. The
tracing interpreter is invoked to start tracing the language interpreter that is
running the user program. Of course the tracing interpreter cannot actually
trace the execution of the C representation of the language interpreter. Instead
it takes the state of the execution of the language interpreter and starts
tracing using a bytecode representation of the language interpreter. That means
there are two "versions" of the language interpreter embedded in the final
executable of the VM: on the one hand it is there as executable machine code, on
the other hand as bytecode for the tracing interpreter. It also means that
tracing is costly as it incurs exactly a double interpretation overhead.

From then on things proceed like described in Section \ref{sect:tracing}. The
tracing interpreter tries to find a loop in the user program, if it finds one it
will produce machine code for that loop and this machine code will be
immediately executed. The machine code is executed until a guard fails. Then the
execution should fall back to normal interpretation by the language interpreter.
This falling back is possibly a complex process, since the guard failure can
have occurred arbitrarily deep in a helper function of the language interpreter,
which would make it hard to rebuild the state of the language interpreter and
let it run from that point (e.g. this would involve building a potentially deep
C stack). Instead the falling back is achieved by a special \emph{fallback
interpreter} which runs the language interpreter and the user program from the
point of the guard failure. The fallback interpreter is essentially a variant of
the tracing interpreter that does not keep a trace. The fallback interpreter
runs until execution reaches a safe point where it is easy to let the C version
of the language interpreter resume its operation. Usually this means that the
fallback interpreter executes at most one bytecode operation of the language
interpreter. After the language interpreter takes over again, the whole process
starts again.

\subsection{Various Issues}

This section will hint at some other implementation issues and optimizations
that we have done that are beyond the scope of this paper (and will be subject
of a later publication).

\textbf{Assembler Backends:} The tracing interpreter uses a well-defined
interface to an assembler backend for code generation. This makes it possible to
easily port the tracing JIT to various architectures (including, we hope, to
virtual machines such as the JVM where backend could generate bytecode at
runtime). At the moment the only implemented backend is a simple 32-bit
Intel-x86 backend.

\textbf{Trace Trees:} This paper ignored the problem of guards that fail in a
large percentage of cases because there are several equally likely paths through
a loop. Just falling back to interpretation in this case is not practicable.
Therefore, if we find a guard that fails often enough, we start tracing from
there and produce efficient machine code for that case, instead of alwayas
falling back to interpretation.

\textbf{Allocation Removal:} A key optimization for making the approach
produce good code for more complex dynamic language is to perform escape
analysis on the loop operation after tracing has been performed. In this way all
objects that are allocated during the loop and don't actually escape the loop do
not need to be allocated on the heap at all but can be exploded into their
respective fields.  This is very helpful for dynamic languages where primitive
types are often boxed, as the constant allocation of intermediate results is
very costly.

\textbf{Optimizing Frame Objects:} One problem with the removal of allocations
is that many dynamic languages are so reflective that they allow the
introspection of the frame object that the interpreter uses to store local
variables (e.g. SmallTalk, Python). This means that intermediate results always
escape because they are stored into the frame object, rendering the allocation
removal optimization ineffective. To remedy this problem we make it possible to
update the frame object lazily only when it is actually accessed from outside of
the code generated by the JIT.

\section{Evaluation}
\label{sect:evaluation}

In this section we try to evaluate the work done so far by looking at some
benchmark numbers. Since the work is not finished, these benchmarks can only be
preliminary. All benchmarking was done on a machine with a 1.4 GHz Pentium M
processor and 1GiB RAM, using Linux 2.6.27.

%- benchmarks
%    - running example
%    - gameboy?

\section{Related Work}

Applying a trace-based optimizer to an interpreter and adding hints to help the
tracer produce better results has been tried before in the context of the DynamoRIO
project \cite{sullivan_dynamic_2003}. This work is conceptually very close to
ours. They achieve the same unrolling of the interpreter loop so that the
unrolled version corresponds to the loops in the user program. However the
approach is greatly hindered by the fact that they trace on the machine code
level and thus have no high-level information available about the interpreter.
This makes it necessary to add quite a large number of hints, because at the
assembler level it is not really visible anymore that e.g. a bytecode string is
really immutable. Also more advanced optimizations like allocation removal would
not be possible with that approach.

The standard approach for automatically producing a compiler for a programming
language given an interpreter for it is that of partial evaluation \cite{XXX},
\cite{XXX}. Conceptually there are some similarities to our work. In partial
evaluation some arguments of the interpreter function are known (static) while
the rest are unknown (dynamic). This separation of arguments is related to our
separation of variables into those that should be part of the position key and
the rest. In partial evaluation all parts of the interpreter that rely only on
static arguments can be constant-folded so that only operations on the dynamic
arguments remain.

Classical partial evaluation has failed to be useful for dynamic language for
much the same reasons why ahead-of-time compilers cannot compile them to
efficient code. If the partial evaluator knows only the program it simply does
not have enough information to produce good code. Therefore some work has been
done to do partial evaluation at runtime. One of the earliest works on runtime
specialisation is Tempo for C \cite{XXX}. However, it is essentially a normal
partial evaluator ``packaged as a library''; decisions about what can be
specialised and how are pre-determined. Another work in this direction is DyC
\cite{grant_dyc_2000}, another runtime specializer for C. Both of these projects
have a similar problem as DynamoRIO.  Targeting the C language makes
higher-level specialisation difficult (e.g.\ \texttt{malloc} can not be
optimized).

There has been some attempts to do \emph{dynamic partial evaluation}, which is
partial evaluation that defers partial evaluation completely to runtime
to make partial evaluation more useful for dynamic languages. This concept was
introduced by Sullivan \cite{sullivan_dynamic_2001} who implemented it for a
small dynamic language based on lambda-calculus. There is some work by one of
the authors to implement a dynamic partial evaluator for Prolog \cite{XXX}.

XXX what else?

\anto{I would cite ourselves (maybe the JIT technical report?) and maybe
  psyco}

\section{Conclusion and Next Steps}

%\begin{verbatim}
%- next steps:
%  - Apply to other things, like smalltalk
%- conclusions
% - advantages + disadvantages in the meta-level approach
% - advantages are that the complex operations that occur in dynamic languages
%   are accessible to the tracer
\cite{bolz_back_2008}

\bigskip

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
