%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}

\usepackage{ifthen}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{ulem}
\usepackage[utf8]{inputenc}

\newboolean{showcomments}
\setboolean{showcomments}{false}
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id: main.tex 19055 2008-06-05 11:20:31Z cfbolz $-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }

\newcommand\cfbolz[1]{\nb{CFB}{#1}}
\newcommand\anto[1]{\nb{ANTO}{#1}}
\newcommand\arigo[1]{\nb{AR}{#1}}
\newcommand\fijal[1]{\nb{FIJAL}{#1}}
\newcommand{\commentout}[1]{}

\normalem

\let\oldcite=\cite

\renewcommand\cite[1]{\ifthenelse{\equal{#1}{XXX}}{[citation~needed]}{\oldcite{#1}}}

% compressing itemize env, in case we need it
\newenvironment{zitemize}% zero - line spacing itemize environment
   {\begin{list}{--}{
   \setlength{\itemsep}{0 pt}
   \setlength{\parsep}{0 pt}
   \setlength{\topsep} {0 pt} }}% the end stuff
   {\end{list}}

\textfloatsep 12pt plus 2pt minus 4pt

\begin{document}

\title{Tracing the Meta-Level: PyPy's Tracing JIT Compiler}

\numberofauthors{3}
\author{
\alignauthor Carl Friedrich Bolz\\
       \affaddr{Heinrich-Heine-Universität Düsseldorf}\\
       \affaddr{STUPS Group}\\
       \affaddr{Germany}\\
       \email{cfbolz@gmx.de}
\alignauthor Antonio Cuni\\
       \affaddr{University of Genova}\\
       \affaddr{DISI}\\
       \affaddr{Italy}\\
       \email{cuni@disi.unige.it}
\and
\alignauthor Armin Rigo\\
       \email{arigo@tunes.org}
\alignauthor Maciej Fijalkowski\\
       \email{fijal@merlinux.eu}
}
\maketitle


%\category{D.3.4}{Programming Languages}{Processors}[code generation,
%interpreters, run-time environments]
%\category{F.3.2}{Logics and Meanings of Programs}{Semantics of Programming
%Languages}[program analysis]

\begin{abstract}
We present techniques for improving the results when a tracing JIT compiler is
applied to an interpreter.XXXAbrupt start - is the relevanz really immediately clear?XXX An unmodified tracing JIT performs not as well as one
would hope when the compiled program is itself a bytecode interpreter. We
examine the reasons for that, and how matters can be improved by adding markers to
the interpreter that help the tracing JIT to improve the results. We evaluate
the techniques by using them both on a small example as well as on a full Python
interpreter. This work has been done in the context of the PyPy project.

\end{abstract}


\section{Introduction}

Dynamic languages have seen a steady rise in popularity in recent years.
JavaScript is increasingly being used to implement full-scale applications
which run within a browser, whereas other dynamic languages (such as Ruby, Perl, Python,
PHP) are used for the server side of many web sites, as well as in areas
unrelated to the web.

One of the often-cited drawbacks of dynamic languages is the performance
penalties they impose. Typically they are slower than statically typed
languages. Even though there has been a lot of research into improving the
performance of dynamic languages (in the SELF project, to name just one example
\cite{XXX}), those techniques are not as widely
used as one would expect. Many dynamic language implementations use completely
straightforward bytecode-interpreters without any advanced implementation
techniques like just-in-time compilation. There are a number of reasons for
this. Most of them boil down to the inherent complexities of using compilation.
Interpreters are simple to implement, understand, extend and port whereas writing a
just-in-time compiler is an error-prone task that is made even harder by the
dynamic features of a language.

A recent approach to getting better performance for dynamic languages is that of
tracing JIT compilers \cite{gal_hotpathvm:effective_2006,
mason_chang_efficient_2007}. Writing a tracing JIT compiler is relatively
simple. It can be added to an existing interpreter for a language,
the interpreter takes over some of the functionality of the compiler and
the machine code generation part can be simplified.

The PyPy project is trying to find approaches to generally ease the
implementation of dynamic languages. It started as a Python implementation in
Python, but has now extended its goals to be generally useful for implementing
other dynamic languages as well. The general approach is to implement an
interpreter for the language in a subset of Python. This subset is chosen in
such a way that programs in it can be compiled into various target environments,
such as C/Posix, the CLI or the JVM. The PyPy project is described in more
detail in Section \ref{sect:pypy}.

In this paper we discuss ongoing work in the PyPy project to improve the
performance of interpreters written with the help of the PyPy toolchain. The
approach is that of a tracing JIT compiler. Contrary to the tracing JITs for dynamic
languages that currently exist, PyPy's tracing JIT operates ``one level down'',
e.g. it traces the execution of the interpreter, as opposed to the execution
of the user program. The fact that the program the tracing JIT compiles is
in our case always an interpreter brings its own set of problems. We describe
tracing JITs and their application to interpreters in Section
\ref{sect:tracing}.  By this approach we hope to arrive at a JIT compiler that can be
applied to a variety of dynamic languages, given an appropriate interpreter for each of them. The
process is not completely automatic but needs a small number of hints from the
interpreter author, to help the tracing JIT. The details of how the process
integrates into the rest of PyPy will be explained in Section
\ref{sect:implementation}. This work is not finished, but has already produced some
promising results, which we will discuss in Section \ref{sect:evaluation}.

The contributions of this paper are:
\begin{zitemize}
\item Applying a tracing JIT compiler to an interpreter.
\item Finding techniques for improving the generated code.
\end{zitemize}


%- dynamic languages important
%- notoriously difficult to achieve good performance
%- even though the techniques exist since a while, not many implementations
%  actually use them
%    - hard to get all corner-cases right
%    - languages evolve
%    - modern dynamic languages are large
%    - open source/research communities don't have that many resources
%
%- PyPy project: trying find approaches to ease the implementation of dynamic
%languages
%- explore general ways to improve on the speed of dynamic languages with reduced
%effort
%- approach: write a tracing JIT that is applicable to many different languages,
%by tracing ``one level done''
%- needs some hints by the interpreter-writer + slightly different optimizations
%- paper will describe the problems of applying a tracing jit to an interpreter
%- different integration needed than a typical tracing jit


\section{The PyPy Project}
\label{sect:pypy}

The PyPy project\footnote{http://codespeak.net/pypy}
\cite{rigo_pypys_2006,carl_friedrich_bolz_to_2007} is an
environment where flexible implementation of dynamic languages can be written.
To implement a dynamic language with PyPy, an interpreter for that language has
to be written in RPython \cite{ancona_rpython:step_2007}. RPython (``Restricted
Python'') is a subset of Python
chosen in such a way that type inference can be performed on it. The language
interpreter can then be translated with the help of PyPy into various target
environments, such as C/Posix, the CLI and the JVM. This is done by a component
of PyPy called the \emph{translation toolchain}.

By writing VMs in a high-level language, we keep the implementation of the
language free of low-level details such as memory management strategy,
threading model or object layout.  These features are automatically added
during the translation process. The process starts by performing control flow
graph construction and type inferences, then followed by a series of steps, each step
transforming the intermediate representation of the program produced by the
previous one until we get the final executable.  The first transformation step
makes details of the Python object model explicit in the intermediate
representation, later steps introducing garbage collection and other low-level
details. As we will see later, this internal representation of the program is
also used as an input for the tracing JIT.


%- original goal: Python interpreter in Python
%- general way to write flexible VMs for dynamic languages
%- interpreter written in RPython, subset of Python to allow type inference
%- translation toolchain
%   - naive forward-propagation type inference
%   - lowering of abstractions
%   - lltype system, monomorphic C-level operations
%   - type system: primitives, pointers to structs and arrays
%   - still assumes presence of GC
%   - can be interpreted in various ways

\section{Tracing JIT Compilers}
\label{sect:tracing}

Tracing JITs are an idea initially explored by the Dynamo project
\cite{bala_dynamo:transparent_2000} in the context of dynamic optimization of
machine code at runtime. The techniques were then successfully applied to Java
VMs \cite{gal_hotpathvm:effective_2006}. It also turned out that they are a
relatively simple way to implement a JIT compiler for a dynamic language
\cite{mason_chang_efficient_2007}. The technique is now
being used by both Mozilla's TraceMonkey JavaScript VM
\cite{andreas_gal_trace-based_2009} and has been tried for Adobe's Tamarin
ActionScript VM \cite{chang_tracing_2009}.

Tracing JITs are built on the following basic assumptions:
\begin{zitemize}
 \item programs spend most of their runtime in loops
 \item several iterations of the same loop are likely to take similar code paths
\end{zitemize}

The basic approach of a tracing JIT is to only generate machine code for the hot
code paths of commonly executed loops and to interpret the rest of the program.
The code for those common loops however should be highly optimized, including
aggressive inlining.

Typically, programs executed by a tracing VMs goes through various phases:
\begin{zitemize}
\item Interpretation/profiling
\item Tracing
\item Code generation
\item Execution of the generated code
\end{zitemize}

The \emph{code generation} phase takes as input the trace generated during
\emph{tracing}.

At first, when the program starts, everything is interpreted.
The interpreter does a small amount of lightweight profiling to establish which loops
are run most frequently. This lightweight profiling is usually done by having a counter on
each backward jump instruction that counts how often this particular backward jump
was executed. Since loops need a backward jump somewhere, this method looks for
loops in the user program.

When a hot loop is identified, the interpreter enters a
special mode, called \emph{tracing mode}. During tracing, the interpreter
records a history of all the operations it executes.

Such a history is called a \emph{trace}: it is a sequential list of
operations, together with their actual operands and results.  By examining the
trace, it is possible to produce highly efficient machine code by generating
only the operations needed.  Being sequential, the trace represents only one
of the many possible paths through the code. To ensure correctness, the trace
contains a \emph{guard} at every possible point where the path could have
followed another direction, for example conditions or indirect/virtual
calls.  When generating the machine code, every guard is turned into a quick check
to guarantee that the path we are executing is still valid.  If a guard fails,
we immediately quit the machine code and continue the execution by falling
back to interpretation.

During tracing, the trace is repeatedly
checked as to whether the interpreter is at a position in the program where it had been
earlier. If this happens, the trace recorded corresponds to a loop
in the interpreted program. At this point, this loop
is turned into machine code by taking the trace and making machine code versions
of all the operations in it. The machine code can then be executed immediately,
starting from the next iteration of the loop, as the machine code represents
exactly the loop that was being interpreted so far.

This process assumes that the path through the loop that was traced is a
``typical'' example of possible paths. Of course
it is possible that later another path through the loop is taken, in which case
one of the guards that were put into the machine code will fail.\footnote{There are more
complex mechanisms in place to still produce more code for the cases of guard
failures \cite{andreas_gal_incremental_2006}, but they are independent of the issues discussed in this
paper.}

It is important to understand how the tracer recognizes that the trace it
recorded so far corresponds to a loop.
This happens when the \emph{position key} is the same as at an earlier
point. The position key describes the position of the execution of the program,
e.g. usually contains things like the function currently being executed and the
program counter position of the tracing interpreter. The tracing interpreter
does not need to check all the time whether the position key already occurred
earlier, but only at instructions that are able to change the position key
to an earlier value, e.g. a backward branch instruction. Note that this is
already the second place where backward branches are treated specially: during
interpretation they are the place where the profiling is performed and where
tracing is started or already existing assembler code executed; during tracing
they are the place where the check for a closed loop is performed.

Let's look at a small example. Take the following (slightly contrived) RPython
code:
{\small
\begin{verbatim}
def f(a, b):
    if b % 46 == 41:
        return a - b
    else:
        return a + b
def strange_sum(n):
    result = 0
    while n >= 0:
        result = f(result, n)
        n -= 1
    return result
\end{verbatim}
}
To trace this, a bytecode form of these functions needs to be introduced that
the tracer understands. The tracer interprets a bytecode that is an encoding of
the intermediate representation of PyPy's translation toolchain after type
inference has been performed.
When the profiler shows
that the \texttt{while} loop in \texttt{strange\_sum} is executed often the
tracing JIT will start to trace the execution of that loop.  The trace would
look as follows:
{\small
\begin{verbatim}
loop_header(result0, n0)
i0 = int_mod(n0, Const(46))
i1 = int_eq(i0, Const(41))
guard_false(i1)
result1 = int_add(result0, n0)
n1 = int_sub(n0, Const(1))
i2 = int_ge(n1, Const(0))
guard_true(i2)
jump(result1, n1)
\end{verbatim}
}
The operations in this sequence are operations of the above-mentioned intermediate
representation (e.g. note that the generic modulo and equality operations in the
function above have been recognized to always take integers as arguments and are thus
rendered as \texttt{int\_mod} and \texttt{int\_eq}). The trace contains all the
operations that were executed in SSA-form \cite{cytron_efficiently_1991} and ends with a jump
to its beginning, forming an endless loop that can only be left via a guard
failure. The call to \texttt{f} is inlined into the trace. Note that the trace
contains only the hot \texttt{else} case of the \texttt{if} test in \texttt{f},
while the other branch is implemented via a guard failure. This trace can then
be converted into machine code and executed.


%- general introduction to tracing
%- assumptions
%- mixed-mode execution environment: interpretation, tracing, compilation,
%  running native code
%- write why tracing jits are particularly well suited for dynamic languages

\subsection{Applying a Tracing JIT to an Interpreter}

The tracing JIT of the PyPy project is atypical in that it is not applied to the
user program, but to the interpreter running the user program. In this section
we will explore what problems this brings, and suggest how to solve them (at least
partially). This means that there are two interpreters involved, and we need appropriate
terminology to distinguish beween them. On the one hand, there is the interpreter that
the tracing JIT uses to perform tracing. This we will call the \emph{tracing
interpreter}. On the other hand, there is the interpreter that runs the
user's programs, which we will call the \emph{language interpreter}. In the
following, we will assume that the language interpreter is bytecode-based. The
program that the language interpreter executes we will call the \emph{user
program} (from the point of view of a VM author, the ``user'' is a programmer
using the VM).

Similarly, we need to distinguish loops at two different levels:
\emph{interpreter loops} are loops \emph{inside} the language interpreter. On
the other hand, \emph{user loops} are loops in the user program.

A tracing JIT compiler finds the hot loops of the program it is compiling. In
our case, this program is the language interpreter. The most important hot interpreter loop
is the bytecode dispatch loop (for many simple
interpreters it is also the only hot loop).  Tracing one iteration of this
loop means that
the recorded trace corresponds to execution of one opcode. This means that the
assumption made by the tracing JIT -- that several iterations of a hot loop
take the same or similar code paths -- is wrong in this case. It is very
unlikely that the same particular opcode is executed many times in a row.
\begin{figure}
\input{code/tlr-paper.py}
\caption{A very simple bytecode interpreter with registers and an accumulator.}
\label{fig:tlr-basic}
\end{figure}
\begin{figure}
{\small
\begin{verbatim}
    MOV_A_R     0   # i = a
    MOV_A_R     1   # copy of 'a'
    
    # 4:
    MOV_R_A     0   # i--
    DECR_A
    MOV_A_R     0    

    MOV_R_A     2   # res += a
    ADD_R_TO_A  1
    MOV_A_R     2
    
    MOV_R_A     0   # if i!=0: goto 4
    JUMP_IF_A   4

    MOV_R_A     2   # return res
    RETURN_A
\end{verbatim}
}
\caption{Example bytecode: Compute the square of the accumulator}
\label{fig:square}
\end{figure}

An example is given in Figure \ref{fig:tlr-basic}. It shows the code of a very
simple bytecode interpreter with 256 registers and an accumulator. The
\texttt{bytecode} argument is a string of bytes, all register and the
accumulator are integers. A program for this interpreter that computes
the square of the accumulator is shown in Figure \ref{fig:square}. If the
tracing interpreter traces the execution of the \texttt{DECR\_A} opcode (whose
integer value is 7), the trace would look as in Figure \ref{fig:trace-normal}.
Because of the guard on \texttt{opcode0}, the code compiled from this trace will
be useful only when executing a long series of \texttt{DECR\_A} opcodes. For all
the other operations the guard will fail, which will mean that performance is
probably not improved at all.

\begin{figure}
\input{code/normal-tracing.txt}
\caption{Trace when executing the \texttt{DECR\_A} opcode}
\label{fig:trace-normal}
\end{figure}

To improve this situation, the tracing JIT could trace the execution of several
opcodes, thus effectively unrolling the bytecode dispatch loop. Ideally, the
bytecode dispatch loop should be unrolled exactly so much that the unrolled version
corresponds to a \emph{user loop}. User loops
occur when the program counter of the \emph{language interpreter} has the
same value several times. This program counter is typically stored in one or several
variables in the language interpreter, for example the bytecode object of the
currently executed function of the user program and the position of the current
bytecode within that.  In the example above, the program counter is represented by 
the \texttt{bytecode} and \texttt{pc} variables.

Since the tracing JIT cannot know which parts of the language interpreter are
the program counter, the author of the language interpreter needs to mark the
relevant variables of the language interpreter with the help of a \emph{hint}.
The tracing interpreter will then effectively add the values of these variables
to the position key. This means that the loop will only be considered to be
closed if these variables that are making up program counter at the language
interpreter level are the same a second time.  Loops found in this way are, by
definition, user loops.

The program counter of the language interpreter can only be the same a
second time after an instruction in the user program sets it to an earlier
value. This happens only at backward jumps in the language interpreter. That
means that the tracing interpreter needs to check for a closed loop only when it
encounters a backward jump in the language interpreter. Again the tracing JIT
cannot know which part of the language interpreter implements backward jumps,
so the author of the language interpreter needs to indicate this with the help
of a hint.

The condition for reusing already existing machine code needs to be adapted to
this new situation. In a classical tracing JIT there is at most one piece of
assembler code per loop of the jitted program, which in our case is the language
interpreter. When applying the tracing JIT to the language interpreter as
described so far, \emph{all} pieces of assembler code correspond to the bytecode
dispatch loop of the language interpreter. However, they correspond to different
paths through the loop and different ways to unroll it. To ascertain which of them to use
when trying to enter assembler code again, the program counter of the language
interpreter needs to be checked. If it corresponds to the position key of one of
the pieces of assembler code, then this assembler code can be executed. This
check again only needs to be performed at the backward branches of the language
interpreter.

The language interpreter uses a similar technique to detect \emph{hot user
loops}: the profiling is done at the backward branches of the user program,
using one counter per seen program counter of the language interpreter.

\begin{figure}
\input{code/tlr-paper-full.py}
\caption{Simple bytecode interpreter with hints applied}
\label{fig:tlr-full}
\end{figure}

Let's look at how hints would need to be applied to the example interpreter
from Figure \ref{fig:tlr-basic}. To apply hints one generally needs a
subclass of \texttt{JitDriver} that lists all the variables of the bytecode
loop. The variables are classified into two groups, red variables and green
variables. The green variables are those that the tracing JIT should consider to
be part of the program counter of the language interpreter. In the case of the
example, the \texttt{pc} variable is obviously part of the program counter.
However, the \texttt{bytecode} variable is also counted as green, since the
\texttt{pc} variable is meaningless without the knowledge of which bytecode
string is currently being interpreted. All other variables are red.

In addition to the classification of the variables, there are two methods of
\texttt{JitDriver} that need to be called. Both of them receive as arguments the
current values of the variables listed in the definition of the driver. The
first one is \texttt{jit\_merge\_point} which needs to be put at the beginning
of the body of the bytecode dispatch loop. The other, more interesting one, is
\texttt{can\_enter\_jit}. This method needs to be called at the end of any
instruction that can set the program counter of the language interpreter to an
earlier value. For the example this is only the \texttt{JUMP\_IF\_A}
instruction, and only if it is actually a backward jump. Here is
where the language interpreter performs profiling to decide
when to start tracing. It is also the place where the tracing JIT checks
whether a loop is closed. This is considered to be the case when the values of
the ``green'' variables are the same as at an earlier call to the
\texttt{can\_enter\_jit} method.

For the small example the hints look like a lot of work. However, the number of
hints is essentially constant no matter how large the interpreter is, which
makes it less significant for larger interpreters.

When executing the Square function of Figure \ref{fig:square}, the profiling
will identify the loop in the square function to be hot, and start tracing. It
traces the execution of the interpreter running the loop of the square function
for one iteration, thus unrolling the interpreter loop of the example
interpreter eight times. The resulting trace can be seen in Figure 
\ref{fig:trace-no-green-folding}.

\begin{figure}
\input{code/no-green-folding.txt}
\caption{Trace when executing the Square function of Figure \ref{fig:square},
with the corresponding bytecodes as comments.}
\label{fig:trace-no-green-folding}
\end{figure}

\subsection{Improving the Result}

The critical problem of tracing the execution of just one opcode has been
solved, the loop corresponds exactly to the loop in the square function.
However, the resulting trace is not optimized enough. Most of its operations are not
actually doing any computation that is part of the square function. Instead,
they manipulate the data structures of the language interpreter. While this is
to be expected, given that the tracing interpreter looks at the execution of the
language interpreter, it would still be an improvement if some of these operations could
be removed.

The simple insight how to improve the situation is that most of the
operations in the trace are actually concerned with manipulating the
bytecode and the program counter. Those are stored in variables that are part of
the position key (they are ``green''), that means that the tracer checks that they
are some fixed value at the beginning of the loop (they may well change over the
course of the loop, though). In the example the check
would be that the \texttt{bytecode} variable is the bytecode string
corresponding to the square function and that the \texttt{pc} variable is
\texttt{4}. Therefore it is possible to constant-fold computations on them away,
as long as the operations are side-effect free. Since strings are immutable in
RPython, it is possible to constant-fold the \texttt{strgetitem} operation. The
\texttt{int\_add} are additions of the green variable \texttt{pc} and a constant
number, so they can be folded away as well.

With this optimization enabled, the trace looks as in Figure
\ref{fig:trace-full}. Now much of the language interpreter is actually gone
from the trace and what is left corresponds very closely to the loop of the
square function. The only vestige of the language interpreter is the fact that
the register list is still used to store the state of the computation. This
could be removed by some other optimization, but is maybe not really all that
bad anyway (in fact we have an experimental optimization that does exactly that,
but it is not yet finished).  Once we get this optimized trace, we can pass it to
the \emph{JIT backend}, which generates the correspondent machine code.

\begin{figure}
\input{code/full.txt}
\caption{Trace when executing the Square function of Figure \ref{fig:square},
with the corresponding opcodes as comments. The constant-folding of operations
on green variables is enabled.}
\label{fig:trace-full}
\end{figure}


%- problem: typical bytecode loops don't follow the general assumption of tracing
%- needs to unroll bytecode loop
%    - how often to unroll
%    - when to start tracing?
%    - unroll exactly so that unrolled loop corresponds to loop of the user
%      program
%- how to improve matters: introducing merge keys
%- constant-folding of operations on green things
%    - similarities to BTA of partial evaluation


\section{Implementation Issues}
\label{sect:implementation}

In this section we will describe some of the practical issues when
implementing the scheme described in the last section in PyPy. In particular
we will describe some of the problems of integrating the various parts with
each other.

\anto{XXX: We shoud clarify the distinction between translation/compilation
  somewhere in the introduction}

The first integration problem is how to \emph{not} integrate the tracing JIT at
all. It is possible to choose when the language interpreter is translated to C
whether the JIT should be built in or not. If the JIT is not enabled, all the
hints that are possibly in the interpreter source are just ignored by the
translation process. In this way, the result of the translation is identical to
that when no hints were present in the interpreter at all.

If the JIT is enabled, things are more interesting. At the moment the JIT can
only be enabled when translating the interpreter to C, but we hope to lift that
restriction in the future. A classical tracing JIT will
interpret the program it is running until a hot loop is identified, at which
point tracing and ultimately assembler generation starts. The tracing JIT in
PyPy is operating on the language interpreter, which is written in RPython. But
RPython programs are statically translatable to C anyway. This means that interpreting the
language interpreter before a hot loop is found is clearly not desirable,
since the overhead of this double-interpretation would be significantly too big
to be practical.

What is done instead is that the language interpreter keeps running as a C
program, until a hot loop in the user program is found. To identify loops, the
C version of the language interpreter is generated in such a way that at the
place that corresponds to the \texttt{can\_enter\_jit} hint profiling is
performed using the program counter of the language interpreter. Apart from this
bit of profiling, the language interpreter behaves in just the same way as
without a JIT.

When a hot user loop is identified, tracing is started. The
tracing interpreter is invoked to start tracing the language interpreter that is
running the user program. Of course the tracing interpreter cannot actually
trace the execution of the C representation of the language interpreter. Instead
it takes the state of the execution of the language interpreter and starts
tracing using a bytecode representation of the language interpreter. That means
there are two ``versions'' of the language interpreter embedded in the final
executable of the VM: on the one hand it is there as executable machine code, on
the other hand as bytecode for the tracing interpreter. It also means that
tracing is costly as it incurs a double interpretation overhead.

From then on things proceed like described in Section \ref{sect:tracing}. The
tracing interpreter tries to find a loop in the user program, if it finds one it
will produce machine code for that loop and this machine code will be
immediately executed. The machine code is executed until a guard fails. Then the
execution should fall back to normal interpretation by the language interpreter.
This falling back is possibly a complex process, since the guard failure can
have occurred arbitrarily deep in a helper function of the language interpreter,
which would make it hard to rebuild the state of the language interpreter and
let it run from that point (e.g. this would involve building a potentially deep
C stack). Instead the falling back is achieved by a special \emph{fallback
interpreter} which runs the language interpreter and the user program from the
point of the guard failure. The fallback interpreter is essentially a variant of
the tracing interpreter that does not keep a trace. The fallback interpreter
runs until execution reaches a safe point where it is easy to let the C version
of the language interpreter resume its operation. Usually this means that the
fallback interpreter executes at most one bytecode operation of the language
interpreter. After the language interpreter takes over again, the whole process
starts again.

\subsection{Various Issues}

This section will look at some other implementation issues and optimizations
that we have done that are beyond the scope of this paper (and will be the subject
of a later publication).

\textbf{Assembler Backends:} The tracing interpreter uses a well-defined
interface to an assembler backend for code generation. This makes it possible to
easily port the tracing JIT to various architectures (including, we hope, to
virtual machines such as the JVM where backend could generate bytecode at
runtime). At the moment the only implemented backend is a 32-bit
Intel-x86 backend.

\textbf{Trace Trees:} This paper ignored the problem of guards that fail often
because there are several equally likely paths through
a loop. Always falling back to interpretation in this case is not practicable.
Therefore, if we find a guard that fails often enough, we start tracing from
there and produce efficient machine code for that case, instead of always
falling back to interpretation.

\textbf{Allocation Removal:} A key optimization for making the approach
produce good code for more complex dynamic language is to perform escape
analysis on the loop operation after tracing has been performed. In this way all
objects that are allocated during the loop and do not actually escape the loop do
not need to be allocated on the heap at all but can be exploded into their
respective fields.  This is very helpful for dynamic languages where primitive
types are often boxed, as the constant allocation of intermediate results is
very costly.

\textbf{Optimizing Frame Objects:} One problem with the removal of allocations
is that many dynamic languages are so reflective that they allow the
introspection of the frame object that the interpreter uses to store local
variables (e.g. SmallTalk, Python). This means that intermediate results always
escape because they are stored into the frame object, rendering the allocation
removal optimization ineffective. To remedy this problem we make it possible to
update the frame object lazily only when it is actually accessed from outside of
the code generated by the JIT.

\section{Evaluation}
\label{sect:evaluation}

In this section we try to evaluate the work done so far by looking at some
benchmark numbers. Since the work is not finished, these benchmarks can only be
preliminary. Benchmarking was done on an otherwise idle machine with a 1.4
GHz Pentium M processor and 1 GB RAM, using Linux 2.6.27. All benchmarks where
run 50 times, each in a newly started process. The first run was ignored. The
final numbers were reached by computing the average of all other runs, the
confidence intervals were computed using a 95\% confidence level. All times
include the running of the tracer and machine code production to measure how
costly those are.

The first round of benchmarks (Figure \ref{fig:bench1}) are timings of the
example interpreter (Figure \ref{fig:tlr-basic}) used in this paper computing
the square of 46340 (the largest number whose square still fits into a 32
bit word) using the bytecode of Figure \ref{fig:square}. The results for various
constellations are as follows:

\textbf{Benchmark 1:} The interpreter translated to C without any JIT inserted at all.

\textbf{Benchmark 2:} The tracing JIT is enabled, but no interpreter-specific
hints are applied. This corresponds to the trace in Figure
\ref{fig:trace-normal}.  The threshold when to consider a loop to be hot is 40
iterations.  As expected, this is not faster than the previous number. It is
even quite a bit slower, probably due to the overheads of the JIT, as well as
non-optimal generated machine code.

\textbf{Benchmark 3:} The hints as in Figure \ref{fig:tlr-full} are applied, which means the loop of
the square function is reflected in the trace. Constant folding of green
variables is disabled though. This corresponds to the trace in Figure
\ref{fig:trace-no-green-folding}. This by alone brings no improvement over the
previous case.

\textbf{Benchmark 4:} Same as before, but with constant folding enabled. This corresponds to the
trace in Figure \ref{fig:trace-full}. This speeds up the square function considerably,
making it about six times faster than the pure interpreter.

\textbf{Benchmark 5:} Same as before, but with the threshold set so high that the tracer is
never invoked to measure the overhead of the profiling. For this interpreter
it seems to be rather large, with 50\% slowdown due to profiling. This is because the interpreter 
is small and the opcodes simple. For larger interpreters (e.g. the Python one) it seems 
likely that the overhead is less significant.

\textbf{Benchmark 6:} Runs the whole computation on the tracing interpreter for estimating the
involved overheads of tracing. The trace is not actually recorded (which would be a
memory problem), so in reality the number is even higher. Due to the double
interpretation, the overhead is huge. It remains to be seen whether that will be
a problem for practical interpreters.

\textbf{Benchmark 7:} For comparison, the time of running the interpreter on top of CPython
(version 2.5.2).

\begin{figure}
\noindent
{\small
\begin{tabular}{|l|r|}
\hline
 &ratio\tabularnewline
\hline
Interpreter compiled to C, no JIT &1\tabularnewline \hline
Normal Trace Compilation &1.20\tabularnewline \hline
Unfolding of Language Interpreter Loop &XXX\tabularnewline \hline
Full Optimizations &0.17\tabularnewline \hline
Profile Overhead &1.51\tabularnewline \hline
Interpreter run by Tracing Interpreter &860.20\tabularnewline \hline
Interpreter run by CPython &256.17\tabularnewline \hline
\end{tabular}
}
\label{fig:bench1}
\caption{Benchmark results of example interpreter computing the square of
46340}
\end{figure}

XXX insert some Python benchmarks

\section{Related Work}

Applying a trace-based optimizer to an interpreter and adding hints to help the
tracer produce better results has been tried before in the context of the DynamoRIO
project \cite{sullivan_dynamic_2003}, which has been a great inspiration for our
work. They achieve the same unrolling of the interpreter loop so that the
unrolled version corresponds to the loops in the user program. However the
approach is greatly hindered by the fact that they trace on the machine code
level and thus have no high-level information available about the interpreter.
This makes it necessary to add quite a large number of hints, because at the
assembler level it is not really visible anymore that e.g. a bytecode string is
immutable. Also more advanced optimizations like allocation removal would
not be possible with that approach.

The standard approach for automatically producing a compiler for a programming
language given an interpreter for it is that of partial evaluation
\cite{futamura_partial_1999, jones_partial_1993}. Conceptually there are some
similarities to our work. In partial
evaluation some arguments of the interpreter function are known (static) while
the rest are unknown (dynamic). This separation of arguments is related to our
separation of variables into those that should be part of the position key and
the rest. In partial evaluation all parts of the interpreter that rely only on
static arguments can be constant-folded so that only operations on the dynamic
arguments remain.

Classical partial evaluation has failed to be useful for dynamic language for
much the same reasons why ahead-of-time compilers cannot compile them to
efficient code. If the partial evaluator knows only the program it simply does
not have enough information to produce good code. Therefore some work has been
done to do partial evaluation at runtime. One of the earliest works on runtime
specialisation is Tempo for C \cite{consel_general_1996, consel_uniform_1996}.
However, it is essentially a normal
partial evaluator ``packaged as a library''; decisions about what can be
specialised and how, are pre-determined. Another work in this direction is DyC
\cite{grant_dyc:expressive_2000}, another runtime specializer for C. Both of these projects
have a problem similar to that of DynamoRIO.  Targeting the C language makes
higher-level specialisation difficult (e.g.\ \texttt{malloc} cannot be
optimized).

There have been some attempts to do \emph{dynamic partial evaluation}, which is
partial evaluation that defers partial evaluation completely to runtime
to make partial evaluation more useful for dynamic languages. This concept was
introduced by Sullivan \cite{sullivan_dynamic_2001} who implemented it for a
small dynamic language based on lambda-calculus. There is some work by one of
the authors to implement a dynamic partial evaluator for Prolog
\cite{carl_friedrich_bolz_automatic_2008}. There are also experiments within the
PyPy project to use dynamic partial evaluation for automatically generating JIT
compilers out of interpreters \cite{armin_rigo_jit_2007}. So far those have not been as
successful as we would like and it seems likely that they will be supplanted
with the work on tracing JITs described here.

\section{Conclusion and Next Steps}

We have shown techniques for improving the results when applying a tracing
JIT to an interpreter. Our first benchmarks indicate that these techniques work
really well on small interpreters and first experiments with PyPy's Python
interpreter make it appear likely that they can be scaled up to realistic
examples.

Of course there is a lot of work still left to do XXX fix this phrase. Various optimizations are not
quite finished. Both tracing and leaving machine code are very slow due to a
double interpretation overhead and we might need techniques for improving those.
Furthermore we need to apply the JIT to the various interpreters that are
written in RPython to evaluate how widely applicable the described techniques
are. Possible targets for such an evaluation would be the SPy-VM, a Smalltalk
implementation \cite{bolz_back_2008}, a Prolog interpreter or PyGirl, a Gameboy
emulator \cite{XXX}; but also not immediately obvious ones, like Python's
regular expression engine. 

If these experiments are successful we hope that we can reach a point where it
becomes unnecessary to write a language specific JIT compiler and just apply a
couple of hints to the interpreter to get reasonably good performance with
relatively little effort.

%\begin{verbatim}
%- next steps:
%  - Apply to other things, like smalltalk
%- conclusions
% - advantages + disadvantages in the meta-level approach
% - advantages are that the complex operations that occur in dynamic languages
%   are accessible to the tracer

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
