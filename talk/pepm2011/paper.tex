\documentclass{sigplanconf}

\usepackage{ifthen}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{ulem}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}

\newboolean{showcomments}
\setboolean{showcomments}{false}
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id: main.tex 19055 2008-06-05 11:20:31Z cfbolz $-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }

\newcommand\cfbolz[1]{\nb{CFB}{#1}}
\newcommand\arigo[1]{\nb{AR}{#1}}
\newcommand\fijal[1]{\nb{FIJAL}{#1}}
\newcommand\david[1]{\nb{DAVID}{#1}}
\newcommand\reva[1]{\nb{Reviewer 1}{#1}}
\newcommand\revb[1]{\nb{Reviewer 2}{#1}}
\newcommand\revc[1]{\nb{Reviewer 3}{#1}}
\newcommand{\commentout}[1]{}

\newcommand\ie{i.e.,\xspace}
\newcommand\eg{e.g.,\xspace}

\normalem

\let\oldcite=\cite

\renewcommand\cite[1]{\ifthenelse{\equal{#1}{XXX}}{[citation~needed]}{\oldcite{#1}}}

%
\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}
%
\begin{document}
\conferenceinfo{PEPM'11,} {XXX}
\CopyrightYear{XXX}
\copyrightdata{XXX}

\title{Escape Analysis and Specialization in a Tracing JIT}

\authorinfo{Carl Friedrich Bolz \and Armin Rigo \and Antion Cuni \and Maciek Fijałkowski}
           {Heinrich-Heine-Universität Düsseldorf, STUPS Group, Germany}
           {cfbolz@gmx.de}

%\numberofauthors{3}
%\author{
%\alignauthor Carl Friedrich Bolz\\
%       \email{cfbolz@gmx.de}
%\alignauthor Michael Leuschel\\
%       \email{leuschel@cs.uni-duesseldorf.de}
%\alignauthor David Schneider\\
%      \email{david.schneider@uni-duesseldorf.de}
%      \sharedaffiliation
%      \affaddr{Heinrich-Heine-Universität Düsseldorf, STUPS Group, Germany}\\
%}

\maketitle
\begin{abstract}
\footnote{This research is partially supported by the BMBF funded project PyJIT (nr. 01QE0913B;
Eureka Eurostars).}
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\category{D.3.4}{Programming Languages}{Processors}[code generation,
interpreters, run-time environments]

\terms
Languages, Performance, Experimentation

\keywords{XXX}%

XXX drop the word "allocation removal" somewhere

\section{Introduction}

The goal of a just-in-time compiler for a dynamic language is obviously to
improve the speed of the language over an implementation of the language that
uses interpretation. The first goal of a JIT is thus to remove the
interpretation overhead, i.e. the overhead of bytecode (or AST) dispatch and the
overhead of the interpreter's data structures, such as operand stack etc. The
second important problem that any JIT for a dynamic language needs to solve is
how to deal with the overhead of boxing of primitive types and of type
dispatching. Those are problems that are usually not present or at least less
severe in statically typed languages.

Boxing of primitive types means that dynamic languages need to be able to handle
all objects, even integers, floats, bools etc. in the same way as user-defined
instances. Thus those primitive types are usually \emph{boxed}, i.e. a small
heap-structure is allocated for them, that contains the actual value. Boxing
primitive types can be very costly, because a lot of common operations,
particularly all arithmetic operations, have to produce a new box, in addition
to the actual computation they do. Because the boxes are allocated on the heap,
producing a lot of them puts pressure on the garbage collector.

Type dispatching is the process of finding the concrete implementation that is
applicable to the objects at hand when doing a generic operation on them. An
example would be the addition of two objects: The addition needs to check what
the concrete objects that should be added are, and choose the implementation
that is fitting for them. Type dispatching is a very common operation in a
dynamic language because no types are known at compile time, so all operations
need it.

A recently popular approach to implementing just-in-time compilers for dynamic
languages is that of a tracing JIT. A tracing JIT often takes the form of an
extension to an existing interpreter, which can be sped up that way. The PyPy
project is an environment for implementing dynamic programming languages. It's
approach to doing so is to straightforwardly implement an interpreter for the
to-be-implemented language, and then use powerful tools to turn the interpreter
into an efficient VM that also contains a just-in-time compiler. This compiler
is automatically generated from the interpreter using partial-evaluation-like
techniques \cite{bolz_tracing_2009}. The PyPy project and its approach to
tracing JIT compilers is described in Section~\ref{sec:Background}

The tracing JIT approach that the PyPy project is taking removes the overhead
of bytecode dispatch. In this paper we want to explain how the traces that are
produced by PyPy's tracing JIT can be optimized to also remove some of the
overhead more closely associated to dynamic languages, such as boxing overhead
and type dispatching. To understand the problem more closely, we analyze the
occurring object lifetimes in Section~\ref{sec:lifetimes}. The most important
technique to achieve this is a form of escape analysis \cite{XXX} that we call
\emph{virtual objects}, which is described in Section~\ref{sec:virtuals}. The
goal of virtual objects is to remove allocations of temporary objects that have
a predictable lifetime and to optimize type dispatching in the process.

The basic approach of virtual objects can then be extended to also be used for
type-specializing the traces that are produced by the tracing JIT
(Section~\ref{sec:crossloop}). In Section~\ref{sec:XXX} we describe some
supporting techniques that are not central to the approach, but are needed to
improve the results. The introduced techniques are evaluated in
Section~\ref{sec:Evaluation} using PyPy's Python interpreter as a case study.

The contributions of this paper are:

\begin{enumerate}
    \item An efficient and effective algorithm for removing objects allocations in a tracing JIT.
    \item XXX
\end{enumerate}

\section{Background}
\label{sec:Background}

\subsection{PyPy}
\label{sub:PyPy}

The work described in this paper was done in the context of the PyPy project
\cite{armin_rigo_pypys_2006}. PyPy is an environment where dynamic languages can
be implemented in a simple yet efficient way. The approach taken when
implementing a language with PyPy is to write an interpreter for the language in
\emph{RPython} \cite{davide_ancona_rpython:_2007}. RPython ("restricted Python")
is a subset of Python chosen in such a way, that type inference becomes
possible. The language interpreter can thus be translated with the help of
PyPy's tools into a VM on the C level. Because the interpreter is written at a
relatively high level, the language implementation is kept free of low-level
details, such as object layout, garbage collection or memory model. Those
aspects of the final VM are woven into the generated code during the translation
to C.

The feature that makes PyPy more than a compiler with a runtime system is it's
support for automated JIT compiler generation \cite{bolz_tracing_2009}. During
the translation to C, PyPy's tools can generate a just-in-time compiler for the
language that the interpreter is implementing. This process is not fully
automatic, but needs to be guided by the language implementer by some
source-code hints.

\subsection{Tracing JIT Compilers}
\label{sub:JIT_background}


traces and bridges

arguments to traces

getting from the interpreter to traces

XXX object model and its reflection in traces (e.g. guard\_class before each method call)

\subsection{Running Example}

For the purpose of this paper, we are going to use a very simple object
model, that just supports an integer and a float type. The objects support only
two operations, \texttt{add}, which adds two objects (promoting ints to floats in a
mixed addition) and \texttt{is\_positive}, which returns whether the number is greater
than zero. The implementation of \texttt{add} uses classical Smalltalk-like
double-dispatching. These classes could be part of the implementation of a very
simple interpreter written in RPython.

\begin{figure}
\begin{verbatim}
class Base(object):
    def add(self, other):
        """ add self to other """
        raise NotImplementedError("abstract base")
    def add__int(self, intother):
        """ add intother to self,
            where intother is an integer """
        raise NotImplementedError("abstract base")
    def add__float(self, floatother):
        """ add floatother to self,
            where floatother is a float """
        raise NotImplementedError("abstract base")
    def is_positive(self):
        """ returns whether self is positive """
        raise NotImplementedError("abstract base")

class BoxedInteger(Base):
    def __init__(self, intval):
        self.intval = intval
    def add(self, other):
        return other.add__int(self.intval)
    def add__int(self, intother):
        return BoxedInteger(intother + self.intval)
    def add__float(self, floatother):
        floatvalue = floatother + float(self.intval)
        return BoxedFloat(floatvalue)
    def is_positive(self):
        return self.intval > 0

class BoxedFloat(Base):
    def __init__(self, floatval):
        self.floatval = floatval
    def add(self, other):
        return other.add__float(self.floatval)
    def add__int(self, intother):
        floatvalue = float(intother) + self.floatval
        return BoxedFloat(floatvalue)
    def add__float(self, floatother):
        return BoxedFloat(floatother + self.floatval)
    def is_positive(self):
        return self.floatval > 0.0
\end{verbatim}
\caption{A simple object model}
\end{figure}

Using these classes to implement arithmetic shows the basic problem that a
dynamic language implementation has. All the numbers are instances of either
\texttt{BoxedInteger} or \texttt{BoxedFloat}, thus they consume space on the
heap. Performing many arithmetic operations produces lots of garbage quickly,
thus putting pressure on the garbage collector. Using double dispatching to
implement the numeric tower needs two method calls per arithmetic operation,
which is costly due to the method dispatch.

To understand the problems more directly, let us consider a simple function
that uses the object model:

\begin{verbatim}
def f(y):
    res = BoxedInteger(0)
    while y.is_positive():
        res = res.add(y).add(BoxedInteger(-100))
        y = y.add(BoxedInteger(-1))
    return res
\end{verbatim}

The loop iterates \texttt{y} times, and computes something in the process. To
understand the reason why executing this function is slow, here is the trace
that is produced by the tracing JIT when executing the function with \texttt{y}
being a \texttt{BoxedInteger}: XXX make it clear that this is really a trace specific for BoxedInteger

\begin{figure}
\begin{verbatim}
    # arguments to the trace: p0, p1
    # inside f: res.add(y)
    guard_class(p1, BoxedInteger)
        # inside BoxedInteger.add
        i2 = getfield_gc(p1, intval)
        guard_class(p0, BoxedInteger)
            # inside BoxedInteger.add__int
            i3 = getfield_gc(p0, intval)
            i4 = int_add(i2, i3)
            p5 = new(BoxedInteger)
                # inside BoxedInteger.__init__
                setfield_gc(p5, i4, intval)
    # inside f: BoxedInteger(-100) 
    p6 = new(BoxedInteger)
        # inside BoxedInteger.__init__
        setfield_gc(p6, -100, intval)

    # inside f: .add(BoxedInteger(-100))
    guard_class(p5, BoxedInteger)
        # inside BoxedInteger.add
        i7 = getfield_gc(p5, intval)
        guard_class(p6, BoxedInteger)
            # inside BoxedInteger.add__int
            i8 = getfield_gc(p6, intval)
            i9 = int_add(i7, i8)
            p10 = new(BoxedInteger)
                # inside BoxedInteger.__init__
                setfield_gc(p10, i9, intval)

    # inside f: BoxedInteger(-1)
    p11 = new(BoxedInteger)
        # inside BoxedInteger.__init__
        setfield_gc(p11, -1, intval)

    # inside f: y.add(BoxedInteger(-1))
    guard_class(p0, BoxedInteger)
        # inside BoxedInteger.add
        i12 = getfield_gc(p0, intval)
        guard_class(p11, BoxedInteger)
            # inside BoxedInteger.add__int
            i13 = getfield_gc(p11, intval)
            i14 = int_add(i12, i13)
            p15 = new(BoxedInteger)
                # inside BoxedInteger.__init__
                setfield_gc(p15, i14, intval)

    # inside f: y.is_positive()
    guard_class(p15, BoxedInteger)
        # inside BoxedInteger.is_positive
        i16 = getfield_gc(p15, intval)
        i17 = int_gt(i16, 0)
    # inside f
    guard_true(i17)
    jump(p15, p10)
\end{verbatim}
\caption{unoptimized trace for the simple object model}
\end{figure}

(indentation corresponds to the stack level of the traced functions).

The trace is inefficient for a couple of reasons. One problem is that it checks
repeatedly and redundantly for the class of the objects around, using a
\texttt{guard\_class} instruction. In addition, some new \texttt{BoxedInteger} instances are
constructed using the \texttt{new} operation, only to be used once and then forgotten
a bit later. In the next section, we will see how this can be improved upon,
using escape analysis.

\section{Object Lifetimes in a Tracing JIT}
\label{sec:lifetimes}

% section Object Lifetimes in a Tracing JIT (end)

To understand the problems that this paper is trying to solve some more, we
first need to understand various cases of object lifetimes that can occur in a
tracing JIT compiler.

\begin{figure}
\includegraphics{figures/obj-lifetime.pdf}

\caption{Object Lifetimes in a Trace}
\label{fig:lifetimes}
\end{figure}

The figure shows a trace before optimization, together with the lifetime of
various kinds of objects created in the trace. It is executed from top to
bottom. At the bottom, a jump is used to execute the same loop another time.
For clarity, the figure shows two iterations of the loop.
The loop is executed until one of the guards in the trace fails, and the
execution is aborted.

Some of the operations within this trace are \texttt{new} operations, which each create a
new instance of some class. These instances are used for a while, e.g. by
calling methods on them, reading and writing their fields. Some of these
instances escape, which means that they are stored in some globally accessible
place or are passed into a function.

Together with the \texttt{new} operations, the figure shows the lifetimes of the
created objects. Objects in category 1 live for a while, and are then just not
used any more. The creation of these objects is removed by the
optimization described in the last section.

Objects in category 2 live for a while and then escape. The optimization of the
last section deals with them too: the \texttt{new} that creates them and
the field accesses are deferred, until the point where the object escapes.

The objects in category 3 and 4 are in principle like the objects in category 1
and 2. They are created, live for a while, but are then passed as an argument
to the \texttt{jump} operation. In the next iteration they can either die (category
3) or escape (category 4).

\section{Escape Analysis in a Tracing JIT}
\label{sec:virtuals}


\subsection{Virtual Objects}

The main insight to improve the code shown in the last section is that some of
the objects created in the trace using a \texttt{new} operation don't survive very
long and are collected by the garbage collector soon after their allocation.
Moreover, they are used only inside the loop, thus we can easily prove that
nobody else in the program stores a reference to them. The
idea for improving the code is thus to analyze which objects never escape the
loop and may thus not be allocated at all.

This process is called \emph{escape analysis}. The escape analysis of
our tracing JIT works by using \emph{virtual objects}: The trace is walked from
beginning to end and whenever a \texttt{new} operation is seen, the operation is
removed and a virtual object is constructed. The virtual object summarizes the
shape of the object that is allocated at this position in the original trace,
and is used by the escape analysis to improve the trace. The shape describes
where the values that would be stored in the fields of the allocated objects
come from. Whenever the optimizer sees a \texttt{setfield} that writes into a virtual
object, that shape summary is thus updated and the operation can be removed.
When the optimizer encounters a \texttt{getfield} from a virtual, the result is read
from the virtual object, and the operation is also removed.

In the example from last section, the following operations would produce two
virtual objects, and be completely removed from the optimized trace:

\begin{verbatim}
p5 = new(BoxedInteger)
setfield_gc(p5, i4, intval)
p6 = new(BoxedInteger)
setfield_gc(p6, -100, intval)
\end{verbatim}


The virtual object stored in \texttt{p5} would know that it is an \texttt{BoxedInteger}, and that
the \texttt{intval} field contains \texttt{i4}, the one stored in \texttt{p6} would know that
its \texttt{intval} field contains the constant -100.

The following operations, that use \texttt{p5} and \texttt{p6} could then be
optimized using that knowledge:

\begin{verbatim}
guard_class(p5, BoxedInteger)
i7 = getfield_gc(p5, intval)
# inside BoxedInteger.add
guard_class(p6, BoxedInteger)
# inside BoxedInteger.add__int
i8 = getfield_gc(p6, intval)
i9 = int_add(i7, i8)
\end{verbatim}

The \texttt{guard\_class} operations can be removed, because the classes of \texttt{p5} and
\texttt{p6} are known to be \texttt{BoxedInteger}. The \texttt{getfield\_gc} operations can be removed
and \texttt{i7} and \texttt{i8} are just replaced by \texttt{i4} and -100. Thus the only
remaining operation in the optimized trace would be:

\begin{verbatim}
i9 = int_add(i4, -100)
\end{verbatim}
    
The rest of the trace is optimized similarly.

So far we have only described what happens when virtual objects are used in
operations that read and write their fields. When the virtual object is used in
any other operation, it cannot stay virtual. For example, when a virtual object
is stored in a globally accessible place, the object needs to actually be
allocated, as it will live longer than one iteration of the loop.

This is what happens at the end of the trace above, when the \texttt{jump} operation
is hit. The arguments of the jump are at this point virtual objects. Before the
jump is emitted, they are \emph{forced}. This means that the optimizers produces code
that allocates a new object of the right type and sets its fields to the field
values that the virtual object has. This means that instead of the jump, the
following operations are emitted:

\begin{verbatim}
p15 = new(BoxedInteger)
setfield_gc(p15, i14, intval)
p10 = new(BoxedInteger)
setfield_gc(p10, i9, intval)
jump(p15, p10)
\end{verbatim}

Note how the operations for creating these two instances has been moved down the
trace. It looks like for these operations we actually didn't win much, because
the objects are still allocated at the end. However, the optimization was still
worthwhile even in this case, because some operations that have been performed
on the forced virtual objects have been removed (some \texttt{getfield\_gc} operations
and \texttt{guard\_class} operations).

The final optimized trace of the example looks like this:

\begin{verbatim}
# arguments to the trace: p0, p1
guard_class(p1, BoxedInteger)
i2 = getfield_gc(p1, intval)
guard_class(p0, BoxedInteger)
i3 = getfield_gc(p0, intval)
i4 = int_add(i2, i3)
i9 = int_add(i4, -100)

guard_class(p0, BoxedInteger)
i12 = getfield_gc(p0, intval)
i14 = int_add(i12, -1)

i17 = int_gt(i14, 0)
guard_true(i17)
p15 = new(BoxedInteger)
setfield_gc(p15, i14, intval)
p10 = new(BoxedInteger)
setfield_gc(p10, i9, intval)
jump(p15, p10)
\end{verbatim}

The optimized trace contains only two allocations, instead of the original five,
and only three \texttt{guard\_class} operations, from the original seven.


%___________________________________________________________________________

\subsection{Summary}

In this section we described how simple escape analysis within the scope of one
loop works. This optimizations reduces the allocation of many intermediate data
structures that become garbage quickly in an interpreter. It also removes a lot
of the type dispatching overhead. In the next section, we will explain how this
optimization can be improved further.

% section Escape Analysis in a Tracing JIT (end)

\section{Escape Analysis Across Loop Boundaries}
\label{sec:crossloop}

In the last section we described how escape analysis can be used to remove
many of the allocations of short-lived objects and many of the type dispatches
that are present in a non-optimized trace. In this section we will improve the
optimization to also handle more cases.

The optimization of the last section considered the passing of an object along a
jump to be equivalent to escaping. It was thus treating objects in category 3
and 4 like those in category 2.

The improved optimization described in this section will make it possible to deal
better with objects in category 3 and 4. This will have two consequences: on
the one hand, more allocations are removed from the trace (which is clearly
good). As a side-effect of this, the traces will also be type-specialized.


%___________________________________________________________________________

\subsection{Optimizing Across the Jump}

\footnote{This section is a bit
science-fictiony. The algorithm that PyPy currently uses is significantly more
complex and much harder than the one that is described here. The resulting
behaviour is very similar, however, so we will use the simpler version (and we
might switch to that at some point in the actual implementation).}

Let's look at the final trace obtained in the last section for the example loop.
The final trace was much better than the original one, because many allocations
were removed from it. However, it also still contained allocations:

\begin{figure}
\includegraphics{figures/step1.pdf}
\end{figure}

The two new \texttt{BoxedIntegers} stored in \texttt{p15} and \texttt{p10} are passed into
the next iteration of the loop. The next iteration will check that they are
indeed \texttt{BoxedIntegers}, read their \texttt{intval} fields and then not use them
any more. Thus those instances are in category 3.

In its current state the loop
allocates two \texttt{BoxedIntegers} at the end of every iteration, that then die
very quickly in the next iteration. In addition, the type checks at the start
of the loop are superfluous, at least after the first iteration.

The reason why we cannot optimize the remaining allocations away is because
their lifetime crosses the jump. To improve the situation, a little trick is
needed. The trace above represents a loop, i.e. the jump at the end jumps to
the beginning. Where in the loop the jump occurs is arbitrary, since the loop
can only be left via failing guards anyway. Therefore it does not change the
semantics of the loop to put the jump at another point into the trace and we
can move the \texttt{jump} operation just above the allocation of the objects that
appear in the current \texttt{jump}. This needs some care, because the arguments to
\texttt{jump} are all currently live variables, thus they need to be adapted.

If we do that for our example trace above, the trace looks like this:
\begin{figure}
\includegraphics{figures/step2.pdf}
\end{figure}

Now the lifetime of the remaining allocations no longer crosses the jump, and
we can run our escape analysis a second time, to get the following trace:
\begin{figure}
\includegraphics{figures/step3.pdf}
\end{figure}

This result is now really good. The code performs the same operations than
the original code, but using direct CPU arithmetic and no boxing, as opposed to
the original version which used dynamic dispatching and boxing.

Looking at the final trace it is also completely clear that specialization has
happened. The trace corresponds to the situation in which the trace was
originally recorded, which happened to be a loop where \texttt{BoxedIntegers} were
used. The now resulting loop does not refer to the \texttt{BoxedInteger} class at
all any more, but it still has the same behaviour. If the original loop had
used \texttt{BoxedFloats}, the final loop would use \texttt{float\_*} operations
everywhere instead (or even be very different, if the object model had
user-defined classes).


%___________________________________________________________________________

\subsection{Entering the Loop}

The approach of placing the \texttt{jump} at some other point in the loop leads to
one additional complication that we glossed over so far. The beginning of the
original loop corresponds to a point in the original program, namely the
\texttt{while} loop in the function \texttt{f} from the last section.

Now recall that in a VM that uses a tracing JIT, all programs start by being
interpreted. This means that when \texttt{f} is executed by the interpreter, it is
easy to go from the interpreter to the first version of the compiled loop.
After the \texttt{jump} is moved and the escape analysis optimization is applied a
second time, this is no longer easily possible.  In particular, the new loop
expects two integers as input arguments, while the old one expected two
instances.

To make it possible to enter the loop directly from the intepreter, there
needs to be some additional code that enters the loop by taking as input
arguments what is available to the interpreter, i.e. two instances. This
additional code corresponds to one iteration of the loop, which is thus
peeled off \cite{XXX}:

\begin{figure}
\includegraphics{figures/step4.pdf}
\end{figure}

XXX optimization particularly effective for chains of operations

%___________________________________________________________________________

\subsection{Summary}

The optimization described in this section can be used to optimize away
allocations in category 3 and improve allocations in category 4, by deferring
them until they are no longer avoidable. A side-effect of these optimizations
is also that the optimized loops are specialized for the types of the variables
that are used inside them.

% section Escape Analysis Across Loop Boundaries (end)

\section{Evaluation}
\label{sec:Evaluation}


\section{Related Work}
\label{sec:related}

\section{Conclusions}
\label{sec:conclusions}

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
